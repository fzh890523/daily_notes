
# rx 和 actor 模型： RxJava、RxGo、protoactor-go



TODO

## RxJava



### ref



#### 深入浅出RxJava系列

* [深入浅出RxJava（一：基础篇）](http://blog.csdn.net/lzyzsd/article/details/41833541) or [local](resource/深入浅出RxJava（一：基础篇） - 大头鬼Bruce - 博客频道 - CSDN.NET.html)
* [深入浅出RxJava(二：操作符)](http://blog.csdn.net/lzyzsd/article/details/44094895)
* [深入浅出RxJava三--响应式的好处](http://blog.csdn.net/lzyzsd/article/details/44891933)
* [深入浅出RxJava四-在Android中使用响应式编程](http://blog.csdn.net/lzyzsd/article/details/45033611)



#### [RxJava基本流程和lift源码分析](http://blog.csdn.net/lzyzsd/article/details/50110355)



#### [RxJava使用场景小结](http://blog.csdn.net/lzyzsd/article/details/50120801)



### RxJava小结

#### RxXX理解

非阻塞io只是在底层（os调度实体）层面解决了阻塞问题，但并不能解决逻辑层面的阻塞：对外部甚至内部的依赖都是存在着“等待”的，比如即使io操作不阻塞，但仍然需要等待io完成。

为了解决`阻塞之痛`，主流的思路有两种：

* 更轻量的阻塞代价： coroutine

  * 优点： 屏蔽细节，代码结构与执行流一致，符合人的思维
  * 缺点： 轻量不等于没有代价，仍然有一定的性能开销，需要不断的优化； 如果不是golang这种`屏蔽阻塞io+machine/process调度`设计的话，存在在coroutine里误用阻塞调用从而坑死worker等情况

* callback

  * 优点： 更贴近真实的执行流，一般性能较好且易优化

  * 缺点： 代码结构复杂（callback hell），不符合人的思维

  * 示例

    ```javascript
    function textProcessing(callback) {
      fs.readFile('somefile.txt', 'utf-8', function (err, contents) {
        if (err) return callback(err);
        //process contents
        contents = contents.toUpperCase();
        fs.readFile('somefile2.txt', 'utf-8', function (err, contents2) {
          if (err) return callback(err);
          contents += contents2;
          fs.writeFile('somefile_concat_uppercase.txt', contents, function (err) {
            if (err) return callback(err);
            callback(null, contents);
          });
        });
      });
    }
    textProcessing(function (err, contents) {
      if (err)
        console.error(err);
    });
    ```

    ​

本质上都是：

1. 执行流/指令位置跳转
   * 对于coroutine来说彻底屏蔽了。 通过coroutine调度实体来....
   * 对于callback来说，通过不断的call/ret
2. 上下文保存/恢复（这里的上下文包括但不限于）
   * 对于coroutine来说，通过（coroutine调度实体种的）coroutine stack来保存
   * 对于callback来说，通过高阶函数、闭包来...



RxXXX可以认为是callback思路的改良，用模式化的方式来进行回调和执行流切换（组织逻辑代码间的关系），在保证性能的同时尽量保证代码的整齐和可读性。 在RxJava种典型的是结合streaming API，使得并发的执行流处理的更加友好。



瞎写的另一种描述（把block -> wait）：

```java
void jumper(function f, continuation c, Object... args) {
  // 这里表示执行完f后继续执行传入的continuation
  // 这里演示了： continuation as first-class
  c(f(args...));
}

void main {
  doSth1();
  doWaitSth2($rest);  // $rest表示"continue from here" -> continuation
  doSth3();
}

@jumper
void doWaitSth2() {
  // ...
}
```

详见下面的引申： [continuation](#引申： continuation)

#### RxJava理解



## 引申： continuation

首先，由上文瞎写的执行流切换伪代码引入...。

TODO

### ref

* [王垠 - 有关continuation](http://docs.huihoo.com/homepage/shredderyin/wiki/ContinuationNotes.html) 不过没说啥，延伸细说了一下CPS（[CPS](http://docs.huihoo.com/homepage/shredderyin/wiki/ContinuationPassingStyle.html) or [local](resource/Continuation Passing Style.html)）和coroutine（[Coroutine Problem](http://docs.huihoo.com/homepage/shredderyin/wiki/CoroutineProblem.html) or [local](resource/Coroutine Problem.html)）
* [CPS Lecture](https://cgi.soic.indiana.edu/~c311/lib/exe/fetch.php?media=cps-notes.scm) or [local](resource/CPS-lecture.txt)
* [Scheme Continuation 三部曲（一）——深入理解 Continuation](http://blog.sina.com.cn/s/blog_4dff871201018wtz.html) or [local](resource/Scheme Continuation 三部曲（一）——深入理解 Continuation_silverbullettt_新浪博客.html)



### 小结/理解



`Continuation 就是一个表达式被求值之后，接下来要做的事情`

> 续上文，“要做的事情”不只是跳转指令地址，还需要“上下文”。
>
> 可以理解为函数指针的增强版，函数指针只是简单的`跳转指令地址`（当然还有一点支持参数和返回值的入栈出栈操作）。
>
> 然后可以继续理解： `function as first-class` -> `continuation as first-class`



#### 执行流的切换方式

至此，执行流的切换有以下几种方式：

1. continuation之间的串联
2. coroutine之间的切换
3. 函数调用（call/ret）
4. GOTO+LABEL




## 引申： call/cc - call-with-current-continuation



TODO

### ref

* [schemewiki - call-with-current-continuation](http://community.schemewiki.org/?call-with-current-continuation) or [local](resource/call-with-current-continuation.html)
* [schemewiki - call-with-current-continuation-for-C-programmers](call-with-current-continuation-for-C-programmers) or [local](resource/call-with-current-continuation-for-C-programmers.html)




### 理解

典型示例如下：

```javascript
util.log("before call/cc");  // 1
var result = call_cc(function (continuation) {
  util.log("in call/cc before set timer");  // 2
  setTimeout(function () {
    util.log("before callback");  // 4（1000ms后）
    continuation("hello world!");
    util.log("after callback");  // 6
  }, 1000);
  util.log("in call/cc after set timer");  // 3
});
util.log("after call/cc " + result);  // 5
```

从代码来看，该call/cc实现的`回到原先的continuation继续执行`需要显式完成。

好处是更加灵活，比如这里可以在timeout的callback里回到continuation； 坏处是无法由调用方决定行为，也即如果call的function不主动回到continuation的话，调用方的后续代码无法继续执行，也即行为由双方共同决定，比较适合回调写法而不是函数对象/指针。



这里还有个疑问： 假设这段代码在func1中，并且在对func1是用普通call执行的，那么是 `5 - 6 - inner ret - func1 ret` 吗？  前面分析了，call_cc只管`保存+跳转`而不管`跳回`，那么continuation是怎么做到`跳回`的呢？



#### call/cc 和 continuation的关系

有上下文可知，call/cc是依赖continuation的，不同的continuation实现会影响call/cc的可用细节。

广义上来说，coroutine、callback（closure）、call/ret等都是不同（完善程度）的continuation。



## 引申： setjmp/longjmp

TODO

### ref

* [浅析C语言的非局部跳转：setjmp和longjmp](http://www.cnblogs.com/lienhua34/archive/2012/04/22/2464859.html)




### 理解/小结

这里引申到...主要是为了探究和continuation的关系，前面ref里提到了：

```
Basically call/cc is like setjmp. It records a dynamic point in the program execution, one which you can return to, ie. jump to, later if you want. setjmp does its thing by filling in a jmp_buf you supply, but call/cc creates a continuation object and calls your nominated bit of code with it.
```

但s/l在原理上是`truncate stack frames`，也即是**单向、决绝**的，一旦做了longjmp就再也无法回转（被truncate的frames内存会在后面的指令中被污染），所以一般按用于**回退、错误处理**等。



此外，s/l跟call/ret的目标不同，并不是（中间可能有多个栈帧，也不可能是）为了获取调用函数的返回值，所以可以认为无法传递调用结果给调用方。

#### 用setjmp/longjmp实现call/cc

功能上看是可以的。

```c
void func1() 
{
	// ... 
  	if(setjmp(jump_buffer) == 0) {  // after setjmp
      func2();  // 
  	} else {  // after longjmp
      // continue rest codes
  	}
}

void func2() {
  // ...
  longjmp(jump_buffer, 1);
  // below codes are unreachable
  // ...
}
```

差不多等于：

```c
void func1() {
  // ...
  callcc(func2);
  // ...
}

void func2() {
  // ...
}
```



同时，还可以：

```c
void func1() 
{
	// ... 
  	if(setjmp(jump_buffer) == 0) {  // after setjmp
      func2();  // 
  	} else {  // after longjmp - maybe unreachable
      // continue rest codes
  	}
}

void func2() {
  // ...
  if(retFlag) {
      longjmp(jump_buffer, 1);
  }
  // below codes may be reachable
  // ...
}
```

原理上，c代码（包括内联汇编的话）差不多是有全部控制权的，所以`switch to`后是否`switch back`是依赖调用函数行为的，和前面js示例里调用函数里不执行continuation类似。



但很明显，这个call/cc跟前面js的例子相比还是有不足： longjmp后就再也回不来了，所以func2无法让func1执行完后继续执行自己的代码，也即`不能把自己作为call_cc的调用方`或者更准确的说`不能把call_cc的两端作为平等的可以互相切换的执行流`。 本质上是因为该实现里`continuation的实现是有缺陷的，两个执行流的上下文数据在一个栈上（jump_buffer上是位置信息，而不是上下文信息），只能单向跳转（truncate）`



同时，前面也提到了，s/l方式无法在为call_cc提供`返回值`这种东西。不过光这一点`可以通过一些迂回的有侵入性的方式解决，比如把jump_buffer封装一下，提供返回值字段等`。但这个侵入性是对于c的，如果是用于实现上层...，则可以消除侵入性了。



## 引申： CPS - continuation passing style

ref：

* [理解Continuation和CPS（Continuation Passing Style）](http://blog.sina.com.cn/s/blog_698213630101bj0q.html)
* [Continuation Passing Style](http://docs.huihoo.com/homepage/shredderyin/wiki/ContinuationPassingStyle.html)




## 引申： coroutine

TODO



ref:

* [Coroutines in C](http://www.chiark.greenend.org.uk/~sgtatham/coroutines.html) or [local](resource/Coroutines in C.html)
* [A Curious Course on Coroutines and Concurrency](http://www.dabeaz.com/coroutines/index.html) or [local](resource/A Curious Course on Coroutines and Concurrency/index.html)
* [Coroutine Problem](http://docs.huihoo.com/homepage/shredderyin/wiki/CoroutineProblem.html) or [local](resource/Coroutine Problem.html)



```
作者：colaghost
链接：https://www.zhihu.com/question/21483863/answer/18379740
来源：知乎
著作权归作者所有，转载请联系作者获得授权。

coroutine你可以将它看成一个用户态的线程（一般来它也提供了入口函数、调用的参数，以及你放置局部变量的栈），只不过它是你自己调度的，而且不同coroutine的切换不需要陷入内核态，效率比较高。
linux有提供了getcontext swapcontext等接口来实现coroutine，windows貌似也有相关的。一般来说coroutine用在异步的场景比较好，异步执行一般需要维护一个状态机，状态的维护需要保存在全局里或者你传进来的参数来，因为每一个状态回调都会重新被调用。有了coroutine(stackfull)的话你可以不用担心这个问题，你可以像写同步的代码那样子，但其实底层还是异步的，只不过你在等待数据时执行的上下文会暂时被保存起来，等到数据来临再将上下文恢复继续执行。还有一种coroutine是stackless，它本质上也是状态机实现的，并不能在它上面让不同的状态共享局部变量，貌似boost.asio.coroutine就是这种。
这里顺便广告一下：colaghost/coroutine_event · GitHub
这是基于libevent简单封装的一个为异步IO提供同步访问的库，包装了accept connect read write这几个接口，同步的过程是利用coroutine的切换来实现的，有兴趣可以看下。
```

```
作者：zhenghui zhou
链接：https://www.zhihu.com/question/21483863/answer/23593082
来源：知乎
著作权归作者所有，转载请联系作者获得授权。

关于协程，knuth解释的最简单明了，协程就是多入多出的子例程，另外从字面组成也非常切贴，就是可以协同运行的例程。

协程本身与线程没有关系，只是如果从调度角度来看的话，相当于一种用户级协作式调度方案（好像windows fiber不完全是协作式的，但我倾向于是协作式才能称为协程）。至于goroutine，就是相当于把调度本身可以分配到多个线程中，或者叫异步调度。我在自己的实现中称之为concurrent coroutine（goroutine可能不能这么叫，已知的协程实现中都不包含真正的并行成分）。

至于还有一些其他概念比如continuation，其实跟协程也没太大关系，只是continuation可以用于实现协程。这是当然的，谁让continuation是一切调用之母呢（is the mother of all function calls，类似有Continuation monad is the mother of all monads）。也有不基于continuation的实现，比如有人贴出的c实现，是基于duff's devices的跳转表（状态机）实现。也可以把两者结合起来用。

我实现了一个还算通用的continuation for c，并在此基础上实现了完整的concurrent coroutine，可以参考：https://github.com/zhouzhenghui/c-sigslot，目前仅提交了continuation部分，包含一个closure实现，可以并行调用，暂未提交完整的concurrent部分以及event-driven部分。
```

```
大概是用户态进行调度的程序片，我重点说下同步实现异步这个事情。
如果把需要自己从用户态调用io去读取buffer称为同步，自己开好buffer让内核去填充称为异步，那么，线程还是协程都无法实现同步方式写异步，只能依靠内核提供异步io函数。
如果仅仅称不需要一直阻塞到io可读等事件，等待可读时可以去调用io函数去处理称为异步。那么多线程就可以实现了，不论你多少个同步io，开一堆线程去做就行了，这样的同步方式写异步，与协程无关，就是线程本身的事情。
如果你要说goroutine做了什么？对于socket io，它调用非阻塞io，使用epoll/iocp等循环取出ready socket，然后适时唤醒阻塞的goroutine，和直接用多线程处理有什么区别？调度更轻量（不进内核态），启动销毁更快。对于文件io，是调用同步io，长系统调用进入前会告诉调度器我要阻塞了，然后新开（也可能只是从池中取）线程跑其他goroutine。然后阻塞返回之后把这个阻塞的线程搞回全局（性能可能不怎么样...linux的aio太差没有用异步，所以只好这样），把线程杀了（或者放入池中）等待以后调度。

作者：李自乐
链接：https://www.zhihu.com/question/21483863/answer/142114886
来源：知乎
著作权归作者所有，转载请联系作者获得授权。
```





# golang实战群关于etcd性能、raid写延迟/吞吐量的讨论



## 讨论记录

```
X:
etcd可以存百万级到千万数量别的key
X:
不过value要小 kb级别
SX:
存是一回事
T:
后面etcd的日志和同步可以考虑分组，这样能提升吞吐量
T:
孟军  越来越好了
X:
存是一回事是什么意思 能解释一下吗
T:
读写频率，同步效率都要考虑
X:
刚才讨论的问题貌似是数目多少 读写是另外一回事了 再者百万级数目的话 更新频率一般也不会每秒更过百分之一 这个就要看具体要求了
T:
百万的百分之一，对于raft来说也很多了
X:
不算多
T:
一般的以太网延时多少？
X:
吞吐和延迟 如果你能承受落盘延迟
X:
200us in dc可以做到
T:
0.2ms么，1s来回也就5000 tps？
X:
你算错了 因为可以batch
X:
另外延迟是落盘为主
T:
不知道你们实测是多少
T:
持久化是异步的
X:
大部分来源系统的raft都是我实现的 raft跑满disk吞吐很容易的
X:
你的理解并不对 持久raft要同步 才能回给用户
T:
知道raft日志是同步的，而且这个可以通过raid cache来提升效率
T:
就问下，etcd 的tps能到多少，你们prd环境到了多少
X:
etcd不是为吞吐优化的 可以跑五万左右
X:
prod 百万key 每秒也就更千分之一
X:
raid 不能提高disk的延迟的
X:
那是提高吞吐的
T:
数量级的提升 好吧
高超:
etcd我们用下来感觉性能很差啊，还有一些莫名其妙的bug，比如client会丢event
T:
以前服务器raid bbu还要经常学习 放点，一旦从wb变成write through ，性能会下一个数量级，基本上
T:
现在都是电容了
X:
raid怎么提升最低延迟 能给我讲讲么
X:
你不太理解吞吐和延迟吧
T:
因为直接写内存，用电池保护
X:
那个跟raid没关系
W:
etcd3 key多影响不大 大多数业务场景满足
W:
@高超@西雅图-Grab 你测试的是etcd2吧
高超:
线上跑的是etcd3
W:
@X@北京 是设置关键词提醒了么？看到etcd就冒出来了
X:
大部分都是client写错了
X:
没设置啊 睡前扫了下微信...
Xargin:
睡前倾听用户反馈
W:
@高超@西雅图-Grab 那按道理不应该丢event啊 
W:
哈哈 
SX:
丢event多半是用法问题
X:
etcd反馈太多了
陈超:
etcd我们用的还行。
高超:
@SX@上海|杭州-有赞 你遇到过这种问题吗？用法上可能存在什么问题？
X:
你们用的是etcd 旧的API吧
高超:
我当时在公司内部力推使用etcd，可没想到后面踩了一堆坑，也只能打落牙齿和血吞了
X:
我怎么没见你们到Github报issue
高超:
之前旧版本api的那个bug我们也碰过了
W:
etcd3 只要客户端不要搞错版本号 应该是不会丢event的 2的话 如果写入速度太快 倒有可能
X:
那不是丢event 是event过期了
X:
我目前没听说丢event的bug
高超:
对，收到过期的event，没收到event是最近的事情
X:
见过的大部分都是client实现问题
高超:
没代码也没什么讨论的基础，gopher大会有缘相见的话再向各位当面讨教吧
X:
过期很正常 你client要处理 之前API event是个划窗
X:
有问题到Github 报bug吧
高超:

X:
@T@途牛 另外跑etcd类似系统 不建议开bbwc 或者说不应该开 而且那个跟raid算法本身并无联系 
T:
 不在一个频道上...
Richard @一米好地:
可以去看看
林足雄:
@高超@西雅图-Grab ETCD 1？
X:
你的频道估计出问题 建议找人修修
T:
延时和吞吐你自己搞错了
T:
wb可以降低演示 但是对顺序写并不提升吞吐量
X:
raid本身不能提高最小延迟 那只是个分盘罢了 有些卡上可以fbwc bbwc 那是两回事
X:
bbwc 可以提高延迟 但是电池单点 回写也会卡吞吐的
T:
wb是直接写内存 竟然说不能降低延时？
X:
那跟raid算法本身无关
X:
你多学习吧
T:
这个在数据库领域都是常识了
我:
说的是不能提高（改善/降低？）最小延迟。 
T:
年轻人真不懂得谦虚啊
X:
你似乎对实现完全不懂
我:
raid 。 不是wb。
X:
wb跟raid两码事
T:
说的是raid卡的bbu wc
SX:
年轻人要谦虚
T:
wb
X:
intel跟我们合作弄这些硬件 具体怎么跑的我很清楚 必要混淆概念
X:
intel跟我们合作弄这些硬件 具体怎么跑的我很清楚 必要混淆概念
X:
不是我不谦虚 你作为用户 也要把事情搞清楚 否则自己要踩坑 如果你是数据库开发人员 那我可能跟你真不在一个频道
SX:
有个内存buffer在，只要buffer没满，延时必然要比直接写低啊
SX:
说的是raid卡的buffer，和raid本身又没关系
X:
@T@途牛 另外跑etcd类似系统 不建议开bbwc 或者说不应该开 而且那个跟raid算法本身并无联系 
T:
先看帖再回帖，我想应该是你理解错了
X:
raid提吞吐
X:
wb 顺序刷盘 靠raid吞吐
X:
所以常连用
X:
raid卡上会有cache
X:
电池 或者 电容刷flash
T:
那就好好说话，wb不都是说的raid卡配置么，你说说看，还有哪儿有
X:
但是跑etcd这样系统
X:
我们并不建议开wb
T:
就是因为对玩过要求高，延时敏感，才要开启wb
T:
对网络
X:
write buffer 实现很多
T:
不建议的理由？有实测数据？
X:
并不意味着raid
SX:
这里说的就是raid的wb
X:
wb 很多raid卡上
X:
是电池的 我刚才说了bb的
X:
电池是单点
X:
挂了的话 raft系统可能无感丢data
T:
一般5节点，难道全坏掉，太牵强
SX:
raft系统本身并不靠单个机器的可用性
X:
dc掉电不常见么
T:
去问问吧，看数据库有几台不开启的
X:
我说的是类etcd系统
T:
那就更应该开启
SX:
dc掉电了，至少电池可以保证持久化吧
SX:
如果只是电池挂掉，也只是退化为wt，电池都是有监控的
X:
呵呵 我们现实见过开wb导致不一致的
SX:
所以你说的不应该开是什么理由？
X:
这个没什么好争论的
X:
你也没数据跟我争论

X:
etcd主要设计目的不是吞吐
T:
但是你给出不建议的开启的建议，没有任何理论和实测依据

X:
我在最开始已经说过了
X:
也不是延迟
X:
一切可能导致不一致的 基本都建议关
X:
你可能跑mysql 或者其它的 会开启
T:
这里给你好好上一课，回去复习下，然后再测测。
X:
两码事
T:
内存还可能不一致呢，哪怕是ecc的
X:
你好好跑你的mysql
SX:
一码事，数据库也是对一致性高要求的系统
SX:
磁盘本身还会出错呢
X:
你只要开了wb就是一个risk
SX:
如果这么容易坏，那是数据存储格式设计问题
X:
你开的原因是为了提高性能
X:
etcd设计并不是为了性能
X:
而是最高的数据保证
X:
只要有可能影响的 一般都关闭
X:
如果还不理解我也没办法了
SX:
所以好好说话不就好了么
SX:
扯那么多还说的不对，就说你理解的不行么
T:
作为etcd的作者，值得肯定和尊重，给大家带了很好的产品，我也有在用。
T:
但是前面你确实有不对的地方，或者说表达不妥的地方，所以说年轻人要谦虚。
X:
你用不用 跟用的对不对并没有联系
X:
我建议你关wb 你也可以开
SX:
能具体说一下，wb造成不一致的结果么？
SX:
实际后果是什么
X:
我年轻与否 跟谦虚不谦虚也没关系 有技术问题就讨论 我没空跟你打嘴架 针对人
SX:
要讨论就好好讨论
X:
实际后果可能无感丢data 我前面已经说过了
T:
我们假设5个etcd物理节点
又假设正好有1个节点wb出现问题，这个节点会出现什么问题
X:
你去机房检查过电池么

T:
这个有监控
SX:
这还需要去机房检查么
SX:
都是有监控的啊
SX:
好好讨论
T:
日志的应用是从磁盘读取的，还是通过内存接收应用
SX:
谭说的那个假设下，会有什么结果
X:
我说的bbwc
X:
和fbwc是两个东西
X:
我们现实中见过bbwc dc掉电
T:
掉电这个节点就T掉好了
X:
首先第一点
X:
etcd不是为了性能优先
X:
而是数据优先
SX:
有fbwc当然更好咯
T:
关键是在不影响数据的情况下，能大幅提升性能，为啥不建议？
X:
在这种情况下开启wb 是牺牲性能换数据 还是牺牲数据换性能 还是说多一个wb 没有增加任何风险
大灰狼Ching:
还在讨论昨晚的话题吗？
X:
开wb 会增加运维 监控负担 增加风险 对于etcd类似系统来说 换来的提升是很小的
SX:
监控谈不上负担
SX:
本来就是统一上的
T:
提升很小是多少？有实测数据吗？
SX:
统一机器配置可以降低运维成本。只要开启以后没明显风险，为啥不开呢
X:
你什么情景下跑etcd必须要开etcd
X:
给我一个具体情景
X:
告诉我你的吞吐需求
SX:
etcd本来也不依赖单机的可靠性，如果单机出问题会造成数据丢失，那说明设计还是不对
X:
我免费给途牛做咨询
X:
没有需求谈性能 我觉得是扯淡
X:
需要开wb
X:
刚才打错了
T:
不用，多谢，这里我并不代表途牛。
我看oracle一般都会给新发布的产品给出一个参考的性能数据
比如tpcc啥的

T:
不用，多谢，这里我并不代表途牛。
我看oracle一般都会给新发布的产品给出一个参考的性能数据
比如tpcc啥的
T:
而且你也没那个本事
X:
你好好用你的oracle吧
M:

M:
淡定不要吵
方圆:

项超:
上面说到的某个节点无感丢data的话，现有的分布式方案都有怎么解决的？
X:
基本上无解啊
X:
理论基础是这样的 可以打时间戳 保证bounded
SX:
我觉着，可以参考下各个数据库都是怎么做的
SX:
应该是可以发现数据已经坏了的
X:
@A@上海-Apple 我建议以后剔除掉对人进行攻击的
X:
应该保证只针对技术讨论
SX:
可别这么说，那你自己也完蛋了
SX:

唐刘:
我们还是来讨论 etcd 吧，@X@北京 你在 comment 跟我说 candidate 必须 apply 所有的 pending committed entries，我在 application 那一层没看到，只在 raft lib 里面看到了，但 raft lib 的 applied log index 已经被提前改了，不会有问题
X:
数据没有写回 不是坏了
X:
etcd那块代码master 改乱了
X:
你softstate发现candidate了
X:
去等一下apply 完结
X:
然后发vote
唐刘:
release 的代码能看吗？master 我还真没找到。。。
A:
相爱相杀，
A:
大家讨论讨论挺好，尽量克制情绪
X:
之前我记得我改过 已经忘记在哪儿了
A:
我也经常这样，没事，抄完之后就好了
X:
etcd代码得问目前的dev了 你开个issue最好了
SX:
问一下，如果是没写回是不是相当于存储上的回滚了？那就和内存中的不一致了，如果说是重启以后，会和其他节点有比对，然后补齐数据？
T:
所以上面说的无感丢数据的原因是什么，没有说清楚

@y@上海-bilibili etcd有wal的 都是batch过的顺序写 随机写也有batch 不过wb开起来 可以帮助突发写降低延迟的 但是在多并发下 提升并不明显 多并发内存本身batch起作用 主要还是部署etcd要尽量避免风险

我:
“在多并发下 提升并不明显”怎么说？ 直观理解起来，wb不是低吞吐随机写场景下对latency改善最明显么？ 等于把hdd当做ssd用。
x:
因为有内存batch
x:
wb的话 如果你只有一个client pingpong的话 
我:
是指应用程序还是指内核的写缓存？ size跟ssd的wb比呢？
x:
如果是hdd 会提升一个数量级 到network bound的
我:
写错了。不是ssd，是raid。
x:
如果是sdd 也会降低几倍把ssd的几百微秒吃了
x:
这个是一个client pingpong的情况
x:
当你多个client并发 CPU batch等等都会引起延迟的 wb节约掉的几毫米并不明显
x:
比如通常1000并发的话 吞吐几万 延迟几个ms wb省去的就不明显了
x:
一般测出来也就是提升20%的样子

SX:
20%已经是很大的提升了
x:
如果是ssd raid 开wb提升就更少了 这种tradeoff 对etcd并不划算
```



## 小结



> 下面引号内容源自引用，大部分是x同学的



* 以太网延时一般dc内可以做到200us，算起来tps 5000，但考虑到 `多连接`+`batch `，实际会高很多。 “延迟是落盘为主”
* “大部分开源系统的raft都是我实现的 raft跑满disk吞吐很容易的”
* “etcd不是为吞吐优化的 可以跑五万左右”
* “raid 不能提高disk的延迟的，那是提高吞吐的”（提高这里为improve）。 多磁头，显然iops和写入速率都会提升，所以可以提高吞吐。
* raid的wb（write buf）或者wc（write cache），可以降低latency。 但在大吞吐顺序写的场景下，会带来频繁的flush，也即会带来较多的latency抖动，一定程度上抵销该优势。同时此场景下寻道开销可以忽略write through性能本来就比较高，所以“跑etcd类似系统 不建议开bbwc 或者说不应该开”




## 引申： BBWC与FBWC

ref：

* [惠普-BBWC与FBWC的区别](http://blog.sina.com.cn/s/blog_82d486440100zbud.html)

```
主机掉电时，HP RAID卡缓存中有 512MB*75%（HP 服务器 缺省75%是写缓存，其余为读缓存） 的数据还未写入到硬盘，
缓存没有供电，则会造成未写入数据丢失。以前使用电池为缓存进行供电，约能支持72小时。
 
电池的缺点是
  1.需要定期充放电，充放电时 write back 被关闭，系统写性能降低
  2.电池寿命约在1年左右，1年后故障率增高
  3.只能供电72小时，时间短

 
--------------------------------------

HP 的缓存供电模式：
1 FBWC=Flash-Based Write Cache(FBWC)    使用 flash 做存储，掉电时有一个大电容供电，将缓存中的内容写入flash. 写入flash 后，永久有效，无72小时限制。
2.BBWC=Battery-Backed Write Cache(BBWC)   使用 电池供电，只能保持72小时的数据。
 
补充：BBWC是靠电池供电来保存数据，FBWC不用电池了，就像U盘那样。

hp的说法是卡上带有一个电容器来给这个flash供电。
```

```
HP于2009年的第四个季度推出了FBWC，FBWC使用闪存（flash）来保存缓存（cache）中的数据，在断电后使用超级电容（super-capacitors）来供电。断电后，BBWC要持续供电以保持cache中的数据，直到电力恢复，而FBWC只需要提供把缓存中的数据备份到闪存所需的电力就行了。FBWC比BBWC有明显的优势，因为一旦FBWC把缓存中的数据全写入到闪存中，就丌存在“电池只能维持48小时”这样的限制了，数据会在下次来电后写入到硬盘中。
```



### raid cache大小

根据一篇[11年的文](http://wenku.baidu.com/link?url=Od78uyE0FrjHEXF_YMJfFpvCYRznvs1RyLT1Y3zSzOp3tYxYppXjnQyrq-q68UvOpwmeRZfREF2TnTk7p7JVIFdCT0scKLseMFz10MUGRIO###)，最简单的RAID卡，一般都包含**几十甚至几百兆**的RAID cache。



## 引申： linux内核flush cache



### 相关内核参数



```
（6）vm.dirty_writeback_centisecs
默认值：499
这个参数会触发pdflush回写进程定期唤醒并将old数据写到磁盘。每次的唤醒的间隔，是以数字100算做1秒。
如果将这项值设为500就相当5秒唤醒pdflush进程。
如果将这项值设为0就表示完全禁止定期回写数据。
（7）vm.dirty_ratio
默认值：40
参数意义：控制一个在产生磁盘写操作的进程开始写出脏数据到内存缓冲区。缓冲区的值大小是系统内存的百分比。增大会使用更多系统内存用于磁盘写缓冲，可以提高系统的写性能。当需要持续、恒定的写入场合时，应该降低该数值。
（8）vm.dirty_expire_centisecs
默认值：2999
参数意义：用来指定内存中数据是多长时间才算脏(dirty)数据。指定的值是按100算做一秒计算。只有当超过这个值后，才会触发内核进程pdflush将dirty数据写到磁盘。
（9）vm.dirty_background_ratio
默认值 ：10
参数意义：控制pdflush后台回写进程开始写出脏数据到系统内存缓冲区。缓冲区的值大小是系统内存的百分比。增大会使用更多系统内存用于磁盘写缓冲，可以提高系统的写性能。当需要持续、恒定的写入场合时，应该降低该数值。
```





# IO系统性能相关



ref：

* [IO系统性能之一：衡量性能的几个指标](http://blog.csdn.net/elf8848/article/details/39927111) or [local](resource/IO系统性能之一：衡量性能的几个指标 - 赵磊的博客-CSDN - 博客频道 - CSDN.NET.html)
* [IO系统性能之二：缓存和RAID如何提高IO](http://blog.csdn.net/elf8848/article/details/39927119) or [local](resource/IO系统性能之二：缓存和RAID如何提高IO - 赵磊的博客-CSDN - 博客频道 - CSDN.NET.html)



# [磁盘性能指标--IOPS 理论](http://elf8848.iteye.com/blog/1731274)



![](pics/seq-access-and-random-access.png)



```
机械硬盘的连续读写性很好， 但随机读写性能很差。这是因为磁头移动至正确的磁道上需要时间，随机读写时，磁头不停的移动，时间都花在了磁头寻道上，所以性能不高。  如下图：

在存储小文件(图片)、OLTP数据库应用时，随机读写性能（IOPS）是最重要指标。
学习它，有助于我们分析存储系统的性能互瓶颈。
下面我们来认识随机读写性能指标--IOPS（每秒的输入输出次数）。
 

磁盘性能指标--IOPS
----------------------------------------------------------
        IOPS (Input/Output Per Second)即每秒的输入输出量(或读写次数)，是衡量磁盘性能的主要指标之一。IOPS是指单位时间内系统能处理的I/O请求数量，一般以每秒处理的I/O请求数量为单位，I/O请求通常为读或写数据操作请求。

    随机读写频繁的应用，如小文件存储(图片)、OLTP数据库、邮件服务器，关注随机读写性能，IOPS是关键衡量指标。

    顺序读写频繁的应用，传输大量连续数据，如电视台的视频编辑，视频点播VOD(Video On Demand)，关注连续读写性能。数据吞吐量是关键衡量指标。

IOPS和数据吞吐量适用于不同的场合：
读取10000个1KB文件，用时10秒  Throught(吞吐量)=1MB/s ，IOPS=1000  追求IOPS
读取1个10MB文件，用时0.2秒  Throught(吞吐量)=50MB/s, IOPS=5  追求吞吐量

磁盘服务时间
--------------------------------------
传统磁盘本质上一种机械装置，如FC, SAS, SATA磁盘，转速通常为5400/7200/10K/15K rpm不等。影响磁盘的关键因素是磁盘服务时间，即磁盘完成一个I/O请求所花费的时间，它由寻道时间、旋转延迟和数据传输时间三部分构成。

寻道时间 Tseek是指将读写磁头移动至正确的磁道上所需要的时间。寻道时间越短，I/O操作越快，目前磁盘的平均寻道时间一般在3－15ms。
旋转延迟 Trotation是指盘片旋转将请求数据所在扇区移至读写磁头下方所需要的时间。旋转延迟取决于磁盘转速，通常使用磁盘旋转一周所需时间的1/2表示。比如，7200 rpm的磁盘平均旋转延迟大约为60*1000/7200/2 = 4.17ms，而转速为15000 rpm的磁盘其平均旋转延迟为2ms。
数据传输时间 Ttransfer是指完成传输所请求的数据所需要的时间，它取决于数据传输率，其值等于数据大小除以数据传输率。目前IDE/ATA能达到133MB/s，SATA II可达到300MB/s的接口数据传输率，数据传输时间通常远小于前两部分消耗时间。简单计算时可忽略。
 
常见磁盘平均物理寻道时间为：
7200转/分的STAT硬盘平均物理寻道时间是9ms
10000转/分的STAT硬盘平均物理寻道时间是6ms
15000转/分的SAS硬盘平均物理寻道时间是4ms
 
常见硬盘的旋转延迟时间为：
7200   rpm的磁盘平均旋转延迟大约为60*1000/7200/2 = 4.17ms
10000 rpm的磁盘平均旋转延迟大约为60*1000/10000/2 = 3ms，
15000 rpm的磁盘其平均旋转延迟约为60*1000/15000/2 = 2ms。


最大IOPS的理论计算方法
--------------------------------------
IOPS = 1000 ms/ (寻道时间 + 旋转延迟)。可以忽略数据传输时间。

7200   rpm的磁盘 IOPS = 1000 / (9 + 4.17)  = 76 IOPS
10000 rpm的磁盘IOPS = 1000 / (6+ 3) = 111 IOPS
15000 rpm的磁盘IOPS = 1000 / (4 + 2) = 166 IOPS


影响测试的因素
-----------------------------------------
实际测量中，IOPS数值会受到很多因素的影响，包括I/O负载特征(读写比例，顺序和随机，工作线程数，队列深度，数据记录大小)、系统配置、操作系统、磁盘驱动等等。因此对比测量磁盘IOPS时，必须在同样的测试基准下进行，即便如此也会产生一定的随机不确定性。


队列深度说明 
NCQ、SCSI TCQ、PATA TCQ和SATA TCQ技术解析 
----------------------------------------
    是一种命令排序技术，一把喂给设备更多的IO请求，让电梯算法和设备有机会来安排合并以及内部并行处理，提高总体效率。
SCSI TCQ的队列深度支持256级
ATA TCQ的队列深度支持32级 （需要8M以上的缓存）
NCQ最高可以支持命令深度级数为32级，NCQ可以最多对32个命令指令进行排序。
    大多数的软件都是属于同步I/O软件，也就是说程序的一次I/O要等到上次I/O操作的完成后才进行，这样在硬盘中同时可能仅只有一个命令，也是无法发挥这个技术的优势，这时队列深度为1。
    随着Intel的超线程技术的普及和应用环境的多任务化，以及异步I/O软件的大量涌现。这项技术可以被应用到了，实际队列深度的增加代表着性能的提高。
在测试时，队列深度为1是主要指标，大多数时候都参考1就可以。实际运行时队列深度也一般不会超过4.


IOPS可细分为如下几个指标：
-----------------------------------------
数据量为n字节，队列深度为k时，随机读取的IOPS
数据量为n字节，队列深度为k时，随机写入的IOPS


IOPS的测试benchmark工具
------------------------------------------
         IOPS的测试benchmark工具主要有Iometer, IoZone, FIO等，可以综合用于测试磁盘在不同情形下的IOPS。对于应用系统，需要首先确定数据的负载特征，然后选择合理的IOPS指标进行测量和对比分析，据此选择合适的存储介质和软件系统。
```





















