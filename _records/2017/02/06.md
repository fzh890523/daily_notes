
# rx 和 actor 模型： RxJava、RxGo、protoactor-go



TODO

## RxJava



### ref



#### 深入浅出RxJava系列

* [深入浅出RxJava（一：基础篇）](http://blog.csdn.net/lzyzsd/article/details/41833541) or [local](resource/深入浅出RxJava（一：基础篇） - 大头鬼Bruce - 博客频道 - CSDN.NET.html)
* [深入浅出RxJava(二：操作符)](http://blog.csdn.net/lzyzsd/article/details/44094895)
* [深入浅出RxJava三--响应式的好处](http://blog.csdn.net/lzyzsd/article/details/44891933)
* [深入浅出RxJava四-在Android中使用响应式编程](http://blog.csdn.net/lzyzsd/article/details/45033611)



#### [RxJava基本流程和lift源码分析](http://blog.csdn.net/lzyzsd/article/details/50110355)



#### [RxJava使用场景小结](http://blog.csdn.net/lzyzsd/article/details/50120801)



### RxJava小结

#### RxXX理解

非阻塞io只是在底层（os调度实体）层面解决了阻塞问题，但并不能解决逻辑层面的阻塞：对外部甚至内部的依赖都是存在着“等待”的，比如即使io操作不阻塞，但仍然需要等待io完成。

为了解决`阻塞之痛`，主流的思路有两种：

* 更轻量的阻塞代价： coroutine

  * 优点： 屏蔽细节，代码结构与执行流一致，符合人的思维
  * 缺点： 轻量不等于没有代价，仍然有一定的性能开销，需要不断的优化； 如果不是golang这种`屏蔽阻塞io+machine/process调度`设计的话，存在在coroutine里误用阻塞调用从而坑死worker等情况

* callback

  * 优点： 更贴近真实的执行流，一般性能较好且易优化

  * 缺点： 代码结构复杂（callback hell），不符合人的思维

  * 示例

    ```javascript
    function textProcessing(callback) {
      fs.readFile('somefile.txt', 'utf-8', function (err, contents) {
        if (err) return callback(err);
        //process contents
        contents = contents.toUpperCase();
        fs.readFile('somefile2.txt', 'utf-8', function (err, contents2) {
          if (err) return callback(err);
          contents += contents2;
          fs.writeFile('somefile_concat_uppercase.txt', contents, function (err) {
            if (err) return callback(err);
            callback(null, contents);
          });
        });
      });
    }
    textProcessing(function (err, contents) {
      if (err)
        console.error(err);
    });
    ```

    ​

本质上都是：

1. 执行流/指令位置跳转
   * 对于coroutine来说彻底屏蔽了。 通过coroutine调度实体来....
   * 对于callback来说，通过不断的call/ret
2. 上下文保存/恢复（这里的上下文包括但不限于）
   * 对于coroutine来说，通过（coroutine调度实体种的）coroutine stack来保存
   * 对于callback来说，通过高阶函数、闭包来...



RxXXX可以认为是callback思路的改良，用模式化的方式来进行回调和执行流切换（组织逻辑代码间的关系），在保证性能的同时尽量保证代码的整齐和可读性。 在RxJava种典型的是结合streaming API，使得并发的执行流处理的更加友好。



瞎写的另一种描述（把block -> wait）：

```java
void jumper(function f, continuation c, Object... args) {
  c(f(args...));
}

void main {
  doSth1();
  doWaitSth2($rest);
  doSth3();
}

@jumper
void doWaitSth2() {
  // ...
}
```

#### RxJava理解



# golang实战群关于etcd性能、raid写延迟/吞吐量的讨论



## 讨论记录

```
X:
etcd可以存百万级到千万数量别的key
X:
不过value要小 kb级别
SX:
存是一回事
T:
后面etcd的日志和同步可以考虑分组，这样能提升吞吐量
T:
孟军  越来越好了
X:
存是一回事是什么意思 能解释一下吗
T:
读写频率，同步效率都要考虑
X:
刚才讨论的问题貌似是数目多少 读写是另外一回事了 再者百万级数目的话 更新频率一般也不会每秒更过百分之一 这个就要看具体要求了
T:
百万的百分之一，对于raft来说也很多了
X:
不算多
T:
一般的以太网延时多少？
X:
吞吐和延迟 如果你能承受落盘延迟
X:
200us in dc可以做到
T:
0.2ms么，1s来回也就5000 tps？
X:
你算错了 因为可以batch
X:
另外延迟是落盘为主
T:
不知道你们实测是多少
T:
持久化是异步的
X:
大部分来源系统的raft都是我实现的 raft跑满disk吞吐很容易的
X:
你的理解并不对 持久raft要同步 才能回给用户
T:
知道raft日志是同步的，而且这个可以通过raid cache来提升效率
T:
就问下，etcd 的tps能到多少，你们prd环境到了多少
X:
etcd不是为吞吐优化的 可以跑五万左右
X:
prod 百万key 每秒也就更千分之一
X:
raid 不能提高disk的延迟的
X:
那是提高吞吐的
T:
数量级的提升 好吧
高超:
etcd我们用下来感觉性能很差啊，还有一些莫名其妙的bug，比如client会丢event
T:
以前服务器raid bbu还要经常学习 放点，一旦从wb变成write through ，性能会下一个数量级，基本上
T:
现在都是电容了
X:
raid怎么提升最低延迟 能给我讲讲么
X:
你不太理解吞吐和延迟吧
T:
因为直接写内存，用电池保护
X:
那个跟raid没关系
W:
etcd3 key多影响不大 大多数业务场景满足
W:
@高超@西雅图-Grab 你测试的是etcd2吧
高超:
线上跑的是etcd3
W:
@X@北京 是设置关键词提醒了么？看到etcd就冒出来了
X:
大部分都是client写错了
X:
没设置啊 睡前扫了下微信...
Xargin:
睡前倾听用户反馈
W:
@高超@西雅图-Grab 那按道理不应该丢event啊 
W:
哈哈 
SX:
丢event多半是用法问题
X:
etcd反馈太多了
陈超:
etcd我们用的还行。
高超:
@SX@上海|杭州-有赞 你遇到过这种问题吗？用法上可能存在什么问题？
X:
你们用的是etcd 旧的API吧
高超:
我当时在公司内部力推使用etcd，可没想到后面踩了一堆坑，也只能打落牙齿和血吞了
X:
我怎么没见你们到Github报issue
高超:
之前旧版本api的那个bug我们也碰过了
W:
etcd3 只要客户端不要搞错版本号 应该是不会丢event的 2的话 如果写入速度太快 倒有可能
X:
那不是丢event 是event过期了
X:
我目前没听说丢event的bug
高超:
对，收到过期的event，没收到event是最近的事情
X:
见过的大部分都是client实现问题
高超:
没代码也没什么讨论的基础，gopher大会有缘相见的话再向各位当面讨教吧
X:
过期很正常 你client要处理 之前API event是个划窗
X:
有问题到Github 报bug吧
高超:

X:
@T@途牛 另外跑etcd类似系统 不建议开bbwc 或者说不应该开 而且那个跟raid算法本身并无联系 
T:
 不在一个频道上...
Richard @一米好地:
可以去看看
林足雄:
@高超@西雅图-Grab ETCD 1？
X:
你的频道估计出问题 建议找人修修
T:
延时和吞吐你自己搞错了
T:
wb可以降低演示 但是对顺序写并不提升吞吐量
X:
raid本身不能提高最小延迟 那只是个分盘罢了 有些卡上可以fbwc bbwc 那是两回事
X:
bbwc 可以提高延迟 但是电池单点 回写也会卡吞吐的
T:
wb是直接写内存 竟然说不能降低延时？
X:
那跟raid算法本身无关
X:
你多学习吧
T:
这个在数据库领域都是常识了
我:
说的是不能提高（改善/降低？）最小延迟。 
T:
年轻人真不懂得谦虚啊
X:
你似乎对实现完全不懂
我:
raid 。 不是wb。
X:
wb跟raid两码事
T:
说的是raid卡的bbu wc
SX:
年轻人要谦虚
T:
wb
X:
intel跟我们合作弄这些硬件 具体怎么跑的我很清楚 必要混淆概念
X:
intel跟我们合作弄这些硬件 具体怎么跑的我很清楚 必要混淆概念
X:
不是我不谦虚 你作为用户 也要把事情搞清楚 否则自己要踩坑 如果你是数据库开发人员 那我可能跟你真不在一个频道
SX:
有个内存buffer在，只要buffer没满，延时必然要比直接写低啊
SX:
说的是raid卡的buffer，和raid本身又没关系
X:
@T@途牛 另外跑etcd类似系统 不建议开bbwc 或者说不应该开 而且那个跟raid算法本身并无联系 
T:
先看帖再回帖，我想应该是你理解错了
X:
raid提吞吐
X:
wb 顺序刷盘 靠raid吞吐
X:
所以常连用
X:
raid卡上会有cache
X:
电池 或者 电容刷flash
T:
那就好好说话，wb不都是说的raid卡配置么，你说说看，还有哪儿有
X:
但是跑etcd这样系统
X:
我们并不建议开wb
T:
就是因为对玩过要求高，延时敏感，才要开启wb
T:
对网络
X:
write buffer 实现很多
T:
不建议的理由？有实测数据？
X:
并不意味着raid
SX:
这里说的就是raid的wb
X:
wb 很多raid卡上
X:
是电池的 我刚才说了bb的
X:
电池是单点
X:
挂了的话 raft系统可能无感丢data
T:
一般5节点，难道全坏掉，太牵强
SX:
raft系统本身并不靠单个机器的可用性
X:
dc掉电不常见么
T:
去问问吧，看数据库有几台不开启的
X:
我说的是类etcd系统
T:
那就更应该开启
SX:
dc掉电了，至少电池可以保证持久化吧
SX:
如果只是电池挂掉，也只是退化为wt，电池都是有监控的
X:
呵呵 我们现实见过开wb导致不一致的
SX:
所以你说的不应该开是什么理由？
X:
这个没什么好争论的
X:
你也没数据跟我争论

X:
etcd主要设计目的不是吞吐
T:
但是你给出不建议的开启的建议，没有任何理论和实测依据

X:
我在最开始已经说过了
X:
也不是延迟
X:
一切可能导致不一致的 基本都建议关
X:
你可能跑mysql 或者其它的 会开启
T:
这里给你好好上一课，回去复习下，然后再测测。
X:
两码事
T:
内存还可能不一致呢，哪怕是ecc的
X:
你好好跑你的mysql
SX:
一码事，数据库也是对一致性高要求的系统
SX:
磁盘本身还会出错呢
X:
你只要开了wb就是一个risk
SX:
如果这么容易坏，那是数据存储格式设计问题
X:
你开的原因是为了提高性能
X:
etcd设计并不是为了性能
X:
而是最高的数据保证
X:
只要有可能影响的 一般都关闭
X:
如果还不理解我也没办法了
SX:
所以好好说话不就好了么
SX:
扯那么多还说的不对，就说你理解的不行么
T:
作为etcd的作者，值得肯定和尊重，给大家带了很好的产品，我也有在用。
T:
但是前面你确实有不对的地方，或者说表达不妥的地方，所以说年轻人要谦虚。
X:
你用不用 跟用的对不对并没有联系
X:
我建议你关wb 你也可以开
SX:
能具体说一下，wb造成不一致的结果么？
SX:
实际后果是什么
X:
我年轻与否 跟谦虚不谦虚也没关系 有技术问题就讨论 我没空跟你打嘴架 针对人
SX:
要讨论就好好讨论
X:
实际后果可能无感丢data 我前面已经说过了
T:
我们假设5个etcd物理节点
又假设正好有1个节点wb出现问题，这个节点会出现什么问题
X:
你去机房检查过电池么

T:
这个有监控
SX:
这还需要去机房检查么
SX:
都是有监控的啊
SX:
好好讨论
T:
日志的应用是从磁盘读取的，还是通过内存接收应用
SX:
谭说的那个假设下，会有什么结果
X:
我说的bbwc
X:
和fbwc是两个东西
X:
我们现实中见过bbwc dc掉电
T:
掉电这个节点就T掉好了
X:
首先第一点
X:
etcd不是为了性能优先
X:
而是数据优先
SX:
有fbwc当然更好咯
T:
关键是在不影响数据的情况下，能大幅提升性能，为啥不建议？
X:
在这种情况下开启wb 是牺牲性能换数据 还是牺牲数据换性能 还是说多一个wb 没有增加任何风险
大灰狼Ching:
还在讨论昨晚的话题吗？
X:
开wb 会增加运维 监控负担 增加风险 对于etcd类似系统来说 换来的提升是很小的
SX:
监控谈不上负担
SX:
本来就是统一上的
T:
提升很小是多少？有实测数据吗？
SX:
统一机器配置可以降低运维成本。只要开启以后没明显风险，为啥不开呢
X:
你什么情景下跑etcd必须要开etcd
X:
给我一个具体情景
X:
告诉我你的吞吐需求
SX:
etcd本来也不依赖单机的可靠性，如果单机出问题会造成数据丢失，那说明设计还是不对
X:
我免费给途牛做咨询
X:
没有需求谈性能 我觉得是扯淡
X:
需要开wb
X:
刚才打错了
T:
不用，多谢，这里我并不代表途牛。
我看oracle一般都会给新发布的产品给出一个参考的性能数据
比如tpcc啥的

T:
不用，多谢，这里我并不代表途牛。
我看oracle一般都会给新发布的产品给出一个参考的性能数据
比如tpcc啥的
T:
而且你也没那个本事
X:
你好好用你的oracle吧
M:

M:
淡定不要吵
方圆:

项超:
上面说到的某个节点无感丢data的话，现有的分布式方案都有怎么解决的？
X:
基本上无解啊
X:
理论基础是这样的 可以打时间戳 保证bounded
SX:
我觉着，可以参考下各个数据库都是怎么做的
SX:
应该是可以发现数据已经坏了的
X:
@A@上海-Apple 我建议以后剔除掉对人进行攻击的
X:
应该保证只针对技术讨论
SX:
可别这么说，那你自己也完蛋了
SX:

唐刘:
我们还是来讨论 etcd 吧，@X@北京 你在 comment 跟我说 candidate 必须 apply 所有的 pending committed entries，我在 application 那一层没看到，只在 raft lib 里面看到了，但 raft lib 的 applied log index 已经被提前改了，不会有问题
X:
数据没有写回 不是坏了
X:
etcd那块代码master 改乱了
X:
你softstate发现candidate了
X:
去等一下apply 完结
X:
然后发vote
唐刘:
release 的代码能看吗？master 我还真没找到。。。
A:
相爱相杀，
A:
大家讨论讨论挺好，尽量克制情绪
X:
之前我记得我改过 已经忘记在哪儿了
A:
我也经常这样，没事，抄完之后就好了
X:
etcd代码得问目前的dev了 你开个issue最好了
SX:
问一下，如果是没写回是不是相当于存储上的回滚了？那就和内存中的不一致了，如果说是重启以后，会和其他节点有比对，然后补齐数据？
T:
所以上面说的无感丢数据的原因是什么，没有说清楚

@y@上海-bilibili etcd有wal的 都是batch过的顺序写 随机写也有batch 不过wb开起来 可以帮助突发写降低延迟的 但是在多并发下 提升并不明显 多并发内存本身batch起作用 主要还是部署etcd要尽量避免风险

我:
“在多并发下 提升并不明显”怎么说？ 直观理解起来，wb不是低吞吐随机写场景下对latency改善最明显么？ 等于把hdd当做ssd用。
x:
因为有内存batch
x:
wb的话 如果你只有一个client pingpong的话 
我:
是指应用程序还是指内核的写缓存？ size跟ssd的wb比呢？
x:
如果是hdd 会提升一个数量级 到network bound的
我:
写错了。不是ssd，是raid。
x:
如果是sdd 也会降低几倍把ssd的几百微秒吃了
x:
这个是一个client pingpong的情况
x:
当你多个client并发 CPU batch等等都会引起延迟的 wb节约掉的几毫米并不明显
x:
比如通常1000并发的话 吞吐几万 延迟几个ms wb省去的就不明显了
x:
一般测出来也就是提升20%的样子

SX:
20%已经是很大的提升了
x:
如果是ssd raid 开wb提升就更少了 这种tradeoff 对etcd并不划算
```



## 小结



> 下面引号内容源自引用，大部分是x同学的



* 以太网延时一般dc内可以做到200us，算起来tps 5000，但考虑到 `多连接`+`batch `，实际会高很多。 “延迟是落盘为主”
* “大部分开源系统的raft都是我实现的 raft跑满disk吞吐很容易的”
* “etcd不是为吞吐优化的 可以跑五万左右”
* “raid 不能提高disk的延迟的，那是提高吞吐的”（提高这里为improve）。 多磁头，显然iops和写入速率都会提升，所以可以提高吞吐。
* raid的wb（write buf）或者wc（write cache），可以降低latency。 但在大吞吐顺序写的场景下，会带来频繁的flush，也即会带来较多的latency抖动，一定程度上抵销该优势。同时此场景下寻道开销可以忽略write through性能本来就比较高，所以“跑etcd类似系统 不建议开bbwc 或者说不应该开”




## 引申： BBWC与FBWC

ref：

* [惠普-BBWC与FBWC的区别](http://blog.sina.com.cn/s/blog_82d486440100zbud.html)

```
主机掉电时，HP RAID卡缓存中有 512MB*75%（HP 服务器 缺省75%是写缓存，其余为读缓存） 的数据还未写入到硬盘，
缓存没有供电，则会造成未写入数据丢失。以前使用电池为缓存进行供电，约能支持72小时。
 
电池的缺点是
  1.需要定期充放电，充放电时 write back 被关闭，系统写性能降低
  2.电池寿命约在1年左右，1年后故障率增高
  3.只能供电72小时，时间短

 
--------------------------------------

HP 的缓存供电模式：
1 FBWC=Flash-Based Write Cache(FBWC)    使用 flash 做存储，掉电时有一个大电容供电，将缓存中的内容写入flash. 写入flash 后，永久有效，无72小时限制。
2.BBWC=Battery-Backed Write Cache(BBWC)   使用 电池供电，只能保持72小时的数据。
 
补充：BBWC是靠电池供电来保存数据，FBWC不用电池了，就像U盘那样。

hp的说法是卡上带有一个电容器来给这个flash供电。
```

```
HP于2009年的第四个季度推出了FBWC，FBWC使用闪存（flash）来保存缓存（cache）中的数据，在断电后使用超级电容（super-capacitors）来供电。断电后，BBWC要持续供电以保持cache中的数据，直到电力恢复，而FBWC只需要提供把缓存中的数据备份到闪存所需的电力就行了。FBWC比BBWC有明显的优势，因为一旦FBWC把缓存中的数据全写入到闪存中，就丌存在“电池只能维持48小时”这样的限制了，数据会在下次来电后写入到硬盘中。
```



### raid cache大小

根据一篇[11年的文](http://wenku.baidu.com/link?url=Od78uyE0FrjHEXF_YMJfFpvCYRznvs1RyLT1Y3zSzOp3tYxYppXjnQyrq-q68UvOpwmeRZfREF2TnTk7p7JVIFdCT0scKLseMFz10MUGRIO###)，最简单的RAID卡，一般都包含**几十甚至几百兆**的RAID cache。



## 引申： linux内核flush cache



### 相关内核参数



```
（6）vm.dirty_writeback_centisecs
默认值：499
这个参数会触发pdflush回写进程定期唤醒并将old数据写到磁盘。每次的唤醒的间隔，是以数字100算做1秒。
如果将这项值设为500就相当5秒唤醒pdflush进程。
如果将这项值设为0就表示完全禁止定期回写数据。
（7）vm.dirty_ratio
默认值：40
参数意义：控制一个在产生磁盘写操作的进程开始写出脏数据到内存缓冲区。缓冲区的值大小是系统内存的百分比。增大会使用更多系统内存用于磁盘写缓冲，可以提高系统的写性能。当需要持续、恒定的写入场合时，应该降低该数值。
（8）vm.dirty_expire_centisecs
默认值：2999
参数意义：用来指定内存中数据是多长时间才算脏(dirty)数据。指定的值是按100算做一秒计算。只有当超过这个值后，才会触发内核进程pdflush将dirty数据写到磁盘。
（9）vm.dirty_background_ratio
默认值 ：10
参数意义：控制pdflush后台回写进程开始写出脏数据到系统内存缓冲区。缓冲区的值大小是系统内存的百分比。增大会使用更多系统内存用于磁盘写缓冲，可以提高系统的写性能。当需要持续、恒定的写入场合时，应该降低该数值。
```





# IO系统性能相关



ref：

* [IO系统性能之一：衡量性能的几个指标](http://blog.csdn.net/elf8848/article/details/39927111) or [local](resource/IO系统性能之一：衡量性能的几个指标 - 赵磊的博客-CSDN - 博客频道 - CSDN.NET.html)
* [IO系统性能之二：缓存和RAID如何提高IO](http://blog.csdn.net/elf8848/article/details/39927119) or [local](resource/IO系统性能之二：缓存和RAID如何提高IO - 赵磊的博客-CSDN - 博客频道 - CSDN.NET.html)



# [磁盘性能指标--IOPS 理论](http://elf8848.iteye.com/blog/1731274)



![](pics/seq-access-and-random-access.png)



```
机械硬盘的连续读写性很好， 但随机读写性能很差。这是因为磁头移动至正确的磁道上需要时间，随机读写时，磁头不停的移动，时间都花在了磁头寻道上，所以性能不高。  如下图：

在存储小文件(图片)、OLTP数据库应用时，随机读写性能（IOPS）是最重要指标。
学习它，有助于我们分析存储系统的性能互瓶颈。
下面我们来认识随机读写性能指标--IOPS（每秒的输入输出次数）。
 

磁盘性能指标--IOPS
----------------------------------------------------------
        IOPS (Input/Output Per Second)即每秒的输入输出量(或读写次数)，是衡量磁盘性能的主要指标之一。IOPS是指单位时间内系统能处理的I/O请求数量，一般以每秒处理的I/O请求数量为单位，I/O请求通常为读或写数据操作请求。

    随机读写频繁的应用，如小文件存储(图片)、OLTP数据库、邮件服务器，关注随机读写性能，IOPS是关键衡量指标。

    顺序读写频繁的应用，传输大量连续数据，如电视台的视频编辑，视频点播VOD(Video On Demand)，关注连续读写性能。数据吞吐量是关键衡量指标。

IOPS和数据吞吐量适用于不同的场合：
读取10000个1KB文件，用时10秒  Throught(吞吐量)=1MB/s ，IOPS=1000  追求IOPS
读取1个10MB文件，用时0.2秒  Throught(吞吐量)=50MB/s, IOPS=5  追求吞吐量

磁盘服务时间
--------------------------------------
传统磁盘本质上一种机械装置，如FC, SAS, SATA磁盘，转速通常为5400/7200/10K/15K rpm不等。影响磁盘的关键因素是磁盘服务时间，即磁盘完成一个I/O请求所花费的时间，它由寻道时间、旋转延迟和数据传输时间三部分构成。

寻道时间 Tseek是指将读写磁头移动至正确的磁道上所需要的时间。寻道时间越短，I/O操作越快，目前磁盘的平均寻道时间一般在3－15ms。
旋转延迟 Trotation是指盘片旋转将请求数据所在扇区移至读写磁头下方所需要的时间。旋转延迟取决于磁盘转速，通常使用磁盘旋转一周所需时间的1/2表示。比如，7200 rpm的磁盘平均旋转延迟大约为60*1000/7200/2 = 4.17ms，而转速为15000 rpm的磁盘其平均旋转延迟为2ms。
数据传输时间 Ttransfer是指完成传输所请求的数据所需要的时间，它取决于数据传输率，其值等于数据大小除以数据传输率。目前IDE/ATA能达到133MB/s，SATA II可达到300MB/s的接口数据传输率，数据传输时间通常远小于前两部分消耗时间。简单计算时可忽略。
 
常见磁盘平均物理寻道时间为：
7200转/分的STAT硬盘平均物理寻道时间是9ms
10000转/分的STAT硬盘平均物理寻道时间是6ms
15000转/分的SAS硬盘平均物理寻道时间是4ms
 
常见硬盘的旋转延迟时间为：
7200   rpm的磁盘平均旋转延迟大约为60*1000/7200/2 = 4.17ms
10000 rpm的磁盘平均旋转延迟大约为60*1000/10000/2 = 3ms，
15000 rpm的磁盘其平均旋转延迟约为60*1000/15000/2 = 2ms。


最大IOPS的理论计算方法
--------------------------------------
IOPS = 1000 ms/ (寻道时间 + 旋转延迟)。可以忽略数据传输时间。

7200   rpm的磁盘 IOPS = 1000 / (9 + 4.17)  = 76 IOPS
10000 rpm的磁盘IOPS = 1000 / (6+ 3) = 111 IOPS
15000 rpm的磁盘IOPS = 1000 / (4 + 2) = 166 IOPS


影响测试的因素
-----------------------------------------
实际测量中，IOPS数值会受到很多因素的影响，包括I/O负载特征(读写比例，顺序和随机，工作线程数，队列深度，数据记录大小)、系统配置、操作系统、磁盘驱动等等。因此对比测量磁盘IOPS时，必须在同样的测试基准下进行，即便如此也会产生一定的随机不确定性。


队列深度说明 
NCQ、SCSI TCQ、PATA TCQ和SATA TCQ技术解析 
----------------------------------------
    是一种命令排序技术，一把喂给设备更多的IO请求，让电梯算法和设备有机会来安排合并以及内部并行处理，提高总体效率。
SCSI TCQ的队列深度支持256级
ATA TCQ的队列深度支持32级 （需要8M以上的缓存）
NCQ最高可以支持命令深度级数为32级，NCQ可以最多对32个命令指令进行排序。
    大多数的软件都是属于同步I/O软件，也就是说程序的一次I/O要等到上次I/O操作的完成后才进行，这样在硬盘中同时可能仅只有一个命令，也是无法发挥这个技术的优势，这时队列深度为1。
    随着Intel的超线程技术的普及和应用环境的多任务化，以及异步I/O软件的大量涌现。这项技术可以被应用到了，实际队列深度的增加代表着性能的提高。
在测试时，队列深度为1是主要指标，大多数时候都参考1就可以。实际运行时队列深度也一般不会超过4.


IOPS可细分为如下几个指标：
-----------------------------------------
数据量为n字节，队列深度为k时，随机读取的IOPS
数据量为n字节，队列深度为k时，随机写入的IOPS


IOPS的测试benchmark工具
------------------------------------------
         IOPS的测试benchmark工具主要有Iometer, IoZone, FIO等，可以综合用于测试磁盘在不同情形下的IOPS。对于应用系统，需要首先确定数据的负载特征，然后选择合理的IOPS指标进行测量和对比分析，据此选择合适的存储介质和软件系统。
```





















