
# AIO 和 各种fnctl的flag 的“兼容性”

## O_NDELAY
会导致 `aio_error` 返回 `EAGAIN`，而这个行为是在 `aio_error` 的额外描述里的。
`*  A positive error number, if the asynchronous I/O operation failed.  This is the same value that would have been stored in the errno variable in the case of a synchronous read(2), write(2), fsync(2), or fdatasync(2) call.`


# errno.h - linux 0.99.3
```c
#ifndef _LINUX_ERRNO_H
#define _LINUX_ERRNO_H

#define EPERM            1      /* Operation not permitted */
#define ENOENT           2      /* No such file or directory */
#define ESRCH            3      /* No such process */
#define EINTR            4      /* Interrupted system call */
#define EIO              5      /* I/O error */
#define ENXIO            6      /* No such device or address */
#define E2BIG            7      /* Arg list too long */
#define ENOEXEC          8      /* Exec format error */
#define EBADF            9      /* Bad file number */
#define ECHILD          10      /* No child processes */
#define EAGAIN          11      /* Try again */
#define ENOMEM          12      /* Out of memory */
#define EACCES          13      /* Permission denied */
#define EFAULT          14      /* Bad address */
#define ENOTBLK         15      /* Block device required */
#define EBUSY           16      /* Device or resource busy */
#define EEXIST          17      /* File exists */
#define EXDEV           18      /* Cross-device link */
#define ENODEV          19      /* No such device */
#define ENOTDIR         20      /* Not a directory */
#define EISDIR          21      /* Is a directory */
#define EINVAL          22      /* Invalid argument */
#define ENFILE          23      /* File table overflow */
#define EMFILE          24      /* Too many open files */
#define ENOTTY          25      /* Not a typewriter */
#define ETXTBSY         26      /* Text file busy */
#define EFBIG           27      /* File too large */
#define ENOSPC          28      /* No space left on device */
#define ESPIPE          29      /* Illegal seek */
#define EROFS           30      /* Read-only file system */
#define EMLINK          31      /* Too many links */
#define EPIPE           32      /* Broken pipe */
#define EDOM            33      /* Math argument out of domain of func */
#define ERANGE          34      /* Math result not representable */
#define EDEADLK         35      /* Resource deadlock would occur */
#define ENAMETOOLONG    36      /* File name too long */
#define ENOLCK          37      /* No record locks available */
#define ENOSYS          38      /* Function not implemented */
#define ENOTEMPTY       39      /* Directory not empty */
#define ELOOP           40      /* Too many symbolic links encountered */
#define EWOULDBLOCK     EAGAIN  /* Operation would block */
#define ENOMSG          42      /* No message of desired type */
#define EIDRM           43      /* Identifier removed */
#define ECHRNG          44      /* Channel number out of range */
#define EL2NSYNC        45      /* Level 2 not synchronized */
#define EL3HLT          46      /* Level 3 halted */
#define EL3RST          47      /* Level 3 reset */
#define ELNRNG          48      /* Link number out of range */
#define EUNATCH         49      /* Protocol driver not attached */
#define ENOCSI          50      /* No CSI structure available */
#define EL2HLT          51      /* Level 2 halted */
#define EBADE           52      /* Invalid exchange */
#define EBADR           53      /* Invalid request descriptor */
#define EXFULL          54      /* Exchange full */
#define ENOANO          55      /* No anode */
#define EBADRQC         56      /* Invalid request code */
#define EBADSLT         57      /* Invalid slot */
#define EDEADLOCK       58      /* File locking deadlock error */
#define EBFONT          59      /* Bad font file format */
#define ENOSTR          60      /* Device not a stream */
#define ENODATA         61      /* No data available */
#define ETIME           62      /* Timer expired */
#define ENOSR           63      /* Out of streams resources */
#define ENONET          64      /* Machine is not on the network */
#define ENOPKG          65      /* Package not installed */
#define EREMOTE         66      /* Object is remote */
#define ENOLINK         67      /* Link has been severed */
#define EADV            68      /* Advertise error */
#define ESRMNT          69      /* Srmount error */
#define ECOMM           70      /* Communication error on send */
#define EPROTO          71      /* Protocol error */
#define EMULTIHOP       72      /* Multihop attempted */
#define EDOTDOT         73      /* RFS specific error */
#define EBADMSG         74      /* Not a data message */
#define EOVERFLOW       75      /* Value too large for defined data type */
#define ENOTUNIQ        76      /* Name not unique on network */
#define EBADFD          77      /* File descriptor in bad state */
#define EREMCHG         78      /* Remote address changed */
#define ELIBACC         79      /* Can not access a needed shared library */
#define ELIBBAD         80      /* Accessing a corrupted shared library */
#define ELIBSCN         81      /* .lib section in a.out corrupted */
#define ELIBMAX         82      /* Attempting to link in too many shared libraries */
#define ELIBEXEC        83      /* Cannot exec a shared library directly */
#define EILSEQ          84      /* Illegal byte sequence */
#define ERESTART        85      /* Interrupted system call should be restarted */
#define ESTRPIPE        86      /* Streams pipe error */
#define EUSERS          87      /* Too many users */
#define ENOTSOCK        88      /* Socket operation on non-socket */
#define EDESTADDRREQ    89      /* Destination address required */
#define EMSGSIZE        90      /* Message too long */
#define EPROTOTYPE      91      /* Protocol wrong type for socket */
#define ENOPROTOOPT     92      /* Protocol not available */
#define EPROTONOSUPPORT 93      /* Protocol not supported */
#define ESOCKTNOSUPPORT 94      /* Socket type not supported */
#define EOPNOTSUPP      95      /* Operation not supported on transport endpoint */
#define EPFNOSUPPORT    96      /* Protocol family not supported */
 #define EAFNOSUPPORT    97      /* Address family not supported by protocol */
 #define EADDRINUSE      98      /* Address already in use */
 #define EADDRNOTAVAIL   99      /* Cannot assign requested address */
 #define ENETDOWN        100     /* Network is down */
 #define ENETUNREACH     101     /* Network is unreachable */
 #define ENETRESET       102     /* Network dropped connection because of reset */
 #define ECONNABORTED    103     /* Software caused connection abort */
 #define ECONNRESET      104     /* Connection reset by peer */
 #define ENOBUFS         105     /* No buffer space available */
 #define EISCONN         106     /* Transport endpoint is already connected */
 #define ENOTCONN        107     /* Transport endpoint is not connected */
 #define ESHUTDOWN       108     /* Cannot send after transport endpoint shutdown */
 #define ETOOMANYREFS    109     /* Too many references: cannot splice */
 #define ETIMEDOUT       110     /* Connection timed out */
 #define ECONNREFUSED    111     /* Connection refused */
 #define EHOSTDOWN       112     /* Host is down */
 #define EHOSTUNREACH    113     /* No route to host */
 #define EALREADY        114     /* Operation already in progress */
 #define EINPROGRESS     115     /* Operation now in progress */
 #define ESTALE          116     /* Stale NFS file handle */
 #define EUCLEAN         117     /* Structure needs cleaning */
 #define ENOTNAM         118     /* Not a XENIX named type file */
 #define ENAVAIL         119     /* No XENIX semaphores available */
 #define EISNAM          120     /* Is a named type file */
 #define EREMOTEIO       121     /* Remote I/O error */

 /* Should never be seen by user programs */
 #define ERESTARTSYS     512
 #define ERESTARTNOINTR  513

 #endif
```

# c语言 - 循环中的变量声明
**只做一次声明，只有一个scope，每次循环都是同一个scope 并且 不会重新初始化**
```c
while(1) {
  printf("do a new aio_read\n");
  ret = aio_read(&my_aiocb);  // commit a read request
  if(ret < 0){
    perror("aio_read");
    exit(1);
  }
  int read_end;  // or int read_end = 0;
  while(!read_end){
    // ... do sth and in end-case set read_end = 1
  }
}
```
> 这个会悲剧

```c
int read_end;
while(1) {
  printf("do a new aio_read\n");
  ret = aio_read(&my_aiocb);  // commit a read request
  if(ret < 0){
    perror("aio_read");
    exit(1);
  }
  read_end = 0;
  while(!read_end){
    // ... do sth and in end-case set read_end = 1
  }
}
```
> 这个OK


# aio一些数据结构

```c
typedef struct sigevent
  {
    sigval_t sigev_value;
    int sigev_signo;
    int sigev_notify;
    void (*sigev_notify_function) (sigval_t);	    /* Function to start.  */
    void *sigev_notify_attributes;		    /* Really pthread_attr_t.*/
  } sigevent_t;
```

```c
/* Type for data associated with a signal.  */
typedef union sigval
  {
    int sival_int;
    void *sival_ptr;
  } sigval_t;
```

# epoll的一些数据结构
```c
typedef union epoll_data {
    void        *ptr;
    int          fd;
    uint32_t     u32;
    uint64_t     u64;
} epoll_data_t;

struct epoll_event {
    uint32_t     events;      /* Epoll events */
    epoll_data_t data;        /* User data variable */
};
```


# select的一些数据结构
```c
struct timeval {
   long    tv_sec;         /* seconds */
   long    tv_usec;        /* microseconds */
};

struct timespec {
   long    tv_sec;         /* seconds */
   long    tv_nsec;        /* nanoseconds */
};
```


# cost/overhead of syscall
```
On IA-32 there are two ways to make a system call:

using int/iret instructions
using sysenter/sysexit instructions
Pure int/iret based system call takes 211 CPU cycles (and even much more on modern processors). Sysenter/sysexit takes 46 CPU ticks. As you can see execution of only a pair of instructions used for system call introduces significant overhead. But any system call implementation involves some work on the kernel side (setup of kernel context, dispatching of the call and its arguments etc.). More or less realistic highly optimized system call will take ~250 and ~100 CPU cycles for int/iret and sysenter/sysexit based system calls respectively. In Linux and Windows it will take ~500 ticks.

In the same time, function call (based on call/ret) have a cost of 2-4 tics + 1 for each argument.

As you can see, overhead introduced by function call is negligible in comparision to the system call cost.

On other hand, if you embed raw system calls in your application, you will make it highly hardware dependent. For example, what if your application with embedded sysenter/sysexit based raw system call will be executed on old PC without these instructions support? In addition your application will be sensitive for system call call convention used by OS.

Such libraries like ntdll.dll and glibc are commonly used, because they provide well-known and hardware independent interface for the system services and hides details of the communication with kernel behind the scene.

Linux and Windows have approximately the same cost of system calls if use the same way of crossing the user/kernel space border (difference will be negligible). Both trying to use fastest way possible on each particular machine. All modern Windows versions starting at least from Windows XP are prepared for sysenter/sysexit. Some old and/or specific versions of Linux can still use int/iret based calls. x64 versions of OSes relies to syscall/sysret instructions which works like the sysenter/sysexit and available as part of AMD64 instructions set.
```

```
Approximate Overhead of System Calls

I'm not sure how this applies to modern computer chips and operating systems, but here is interesting research from Liedtke in 1995 and 1997 showing the overhead of system calls:
For measuring the system-call overhead, getpid, the shortest Linux system call, was examined. To measure its cost under ideal circumstances, it was repeatedly invoked in a tight loop. Table 2 shows the consumed cycles and the time per invocation derived from the cycle numbers. The numbers were obtained using the cycle counter register of the Pentium processor.
Linux = 223 cycles = 1.68 µs (133MHz Pentium)
The Performance of µ-Kernel-Based Systems, Liedtke et al, 1997, http://os.inf.tu-dresden.de/pubs/sosp97/.
It is widely believed that switching between kernel and user mode, between address spaces and between threads is inherently expensive. Some measurements seem to support this belief.
Ousterhout [1990] measured the costs for executing the "null" kernel call getpid. Since the real getpid operation consists only of a few loads and stores, this method measures the basic costs of a kernel call. Normalized to a hypothetical machine with 10 MIPS rating... he showed that most machines need 20-30 μs per getpid, one required even 63 μs. Corroborating these results, we measured 18 μs per Mach μ-kernel call get self thread. In fact, the measured kernel-call costs are high.
For analyzing the measured costs, our argument is based on a 486 (50 MHz) processor. We take an x86 processor, because kernel-user mode switches are extremely expensive on these processors. In contrast to the worst case processor, we use a best-case measurement for discussion, 18 μs for Mach on a 486/50.
The measured costs per kernel call are 18x50 = 900 cycles. The bare machine instruction for entering kernel mode costs 71 cycles, followed by an additional 36 cycles for returning to user mode. These two instructions switch between the user and kernel stack and push/pop flag register and instruction pointer. 107 cycles (about 2 μs) is therefore a lower bound on kernel/user mode switches. The remaining 800 or more cycles are pure kernel overhead. By this term, we denote all cycles which are solely due to the construction of the kernel, nevermind whether they are spent in executing instructions (800 cycles ~ 500 instructions) or in cache and TLB misses (800 cycles ~ 270 primary cache misses ~ 90 TLB misses). We have to conclude that the measured kernels do a lot of work when entering and exiting the kernel. Note that this work by definition has no net effect.
On µ-Kernel Construction, Liedtke, 1995, http://os.ibds.kit.edu/downloads/publ_1995_liedtke_ukernel-construction.pdf.
```

```
today i was comparing the performance of some netmap-related code
on FreeBSD and Linux (RELENG_9 vs 3.2) and i was surprised to see that
our system calls are significantly slower.
On comparable hardware (i7-2600k vs E5-1650) the syscall
getppid() takes about 95ns on FreeBSD and 38ns on linux.

(i make sure not to use gettimeofday(), which in linux is through vdso,
and getpid(), which is cached by glibc).

Any idea on why there is this difference and whether/how
we can reduce it ?

cheers
luigi

https://lists.freebsd.org/pipermail/freebsd-current/2012-November/038019.html
```

# glibc syscall example
```c
/* HPPA implements syscall() in 'C'; the assembler version would
   typically be in syscall.S. Also note that we have INLINE_SYSCALL,
   INTERNAL_SYSCALL, and all the generated pure assembly syscall wrappers.
   How often the function is used is unknown. */

long int
syscall (long int __sysno, ...)
{
  /* FIXME: Keep this matching INLINE_SYSCALL for hppa */
  va_list args;
  long int arg0, arg1, arg2, arg3, arg4, arg5;
  long int __sys_res;

  /* Load varargs */
  va_start (args, __sysno);
  arg0 = va_arg (args, long int);
  arg1 = va_arg (args, long int);
  arg2 = va_arg (args, long int);
  arg3 = va_arg (args, long int);
  arg4 = va_arg (args, long int);
  arg5 = va_arg (args, long int);
  va_end (args);

  {
    LOAD_ARGS_6 (arg0, arg1, arg2, arg3, arg4, arg5)
    register unsigned long int __res asm("r28");
    PIC_REG_DEF
    LOAD_REGS_6
    asm volatile (SAVE_ASM_PIC
		  "	ble  0x100(%%sr2, %%r0)	\n"
		  "	copy %1, %%r20		\n"
		  LOAD_ASM_PIC
		  : "=r" (__res)
		  : "r" (__sysno) PIC_REG_USE ASM_ARGS_6
		  : "memory", CALL_CLOB_REGS CLOB_ARGS_6);
    __sys_res = __res;
  }
  if ((unsigned long int) __sys_res >= (unsigned long int) -4095)
    {
      __set_errno (-__sys_res);
      __sys_res = -1;
    }
  return __sys_res;
}
```

# calling conventions for UNIX & Linux system calls on x86-64
```
I verified these using GNU Assembler (gas) on Linux.

Kernel Interface
x86-32 Linux System Call convention:

In x86-32 parameters for Linux system call are passed using registers. %eax for syscall_number. %ebx, %ecx, %edx, %esi, %edi, %ebp are used for passing 6 parameters to system calls.

The return value is in %eax. All other registers (including EFLAGS) are preserved across the int 0x80.

I took following snippet from the Linux Assembly Tutorial but I'm doubtful about this. If any one can show an example, it would be great.

If there are more than six arguments,  %ebx must contain the memory location where the list of arguments is stored - but don't worry about this because it's unlikely that you'll use a syscall with more than six arguments.
For an example and a little more reading, refer to http://www.int80h.org/bsdasm/#alternate-calling-convention

There is faster way to make 32bit system calls: using sysenter. The kernel maps a page of memory into every process (the vdso), with the user-space side of the sysenter, which has to cooperate with the kernel for it to be able to find the return address. arg to register mapping is the same as for int 0x80, but instead of that instruction, code should call a function in the vdso. (TODO: update this with a link and/or specific info).

x86-32 [Free|Open|Net|DragonFly]BSD UNIX System Call convention:

Parameters are passed on the stack. Push the parameters (last parameter pushed first) on to the stack. Then push an additional 32-bit of dummy data (Its not actually dummy data. refer to following link for more info) and then give a system call instruction int $0x80

http://www.int80h.org/bsdasm/#default-calling-convention

x86-64 Linux & *BSD System Call convention:

Refer to section: "A.2 AMD64 Linux Kernel Conventions" of System V Application Binary Interface AMD64 Architecture Processor Supplement

Here is the snippet from this section:

User-level applications use as integer registers for passing the sequence %rdi, %rsi, %rdx, %rcx, %r8 and %r9. The kernel interface uses %rdi, %rsi, %rdx, %r10, %r8 and %r9.
A system-call is done via the syscall instruction. The kernel destroys registers %rcx and %r11.
The number of the syscall has to be passed in register %rax.
System-calls are limited to six arguments, no argument is passed directly on the stack.
Returning from the syscall, register %rax contains the result of the system-call. A value in the range between -4095 and -1 indicates an error, it is -errno.
Only values of class INTEGER or class MEMORY are passed to the kernel.
User Interface
x86-32 Function Calling convention:

In x86-32 parameters were passed on stack. Last parameter was pushed first on to the stack until all parameters are done and then call instruction was executed. This is used for calling C library (libc) functions on Linux from assembly.

x86-64 Function Calling convention:

I guess because of the reason that we have so many general purpose registers and higher width other registers the function calling mechanism is changed. In this new mechanism. First the parameters are divided into classes. The class of each parameter determines the manner in which it is passed to the called function.

For complete information refer to : "3.2 Function Calling Sequence" of System V Application Binary Interface AMD64 Architecture Processor Supplement which reads, in part:

Once arguments are classified, the registers get assigned (in left-to-right order) for passing as follows:

If the class is MEMORY, pass the argument on the stack.
If the class is INTEGER, the next available register of the sequence %rdi, %rsi, %rdx, %rcx, %r8 and %r9 is used
so %rdi, %rsi, %rdx, %rcx, %r8 and %r9 are the registers in order used to pass parameters to any libc function from assembly. %rdi is used for first parameter. %rsi for 2nd, %rdx for 3rd and so on. Then call instruction should be given.

If parameters are more than 6 then 7th parameter onwards is passed on the stack.

```
[stackoverflow-what-are-the-calling-conventions-for-unix-linux-system-calls-on-x86-64](http://stackoverflow.com/questions/2535989/what-are-the-calling-conventions-for-unix-linux-system-calls-on-x86-64)


# linux assembly interface: int 0x80, sysenter
## int 0x80
On both Linux x86 and Linux x86_64 systems you can make a syscall by calling interrupt 0x80 using the int $0x80 command. Parameters are passed by setting the general purpose registers as following:

| Syscall #     | Param 1     | Param 2     | Param 3     | Param 4     | Param 5     | Param 6     |
| :------------- | :------------- |
| eax       | ebx       | ecx       | edx       | esi       | edi       | ebp       |

| Return value     |
| :------------- |
| eax       |
The syscall numbers are described in the Linux source file arch/x86/include/asm/unistd_32.h.

All registers are preserved during the syscall.

## syscall
The x86_64 architecture introduced a dedicated instruction to make a syscall. It does not access the interrupt descriptor table and is faster. Parameters are passed by setting the general purpose registers as following:

| Syscall #     | Param 1     | Param 2     | Param 3     | Param 4     | Param 5     | Param 6     |
| :------------- | :------------- |
| rax       | rdi       | rsi       | rdx       | r10       | r8       | r9       |

| Return value     |
| :------------- |
| rax       |


# vsyscall; sysenter,sysexit; syscall,sysret
```
VDSO(Virtual Dynamically-linked Shared Object)是个很有意思的东西, 它将内核态的调用映射到用户态的地址空间中, 使得调用开销更小, 路径更好.

开销更小比较容易理解, 那么路径更好指的是什么呢? 拿x86下的系统调用举例, 传统的int 0x80有点慢, Intel和AMD分别实现了sysenter, sysexit和syscall, sysret, 即所谓的快速系统调用指令, 使用它们更快, 但是也带来了兼容性的问题. 于是Linux实现了vsyscall, 程序统一调用vsyscall, 具体的选择由内核来决定. 而vsyscall的实现就在VDSO中.

Linux(kernel 2.6 or upper)环境下执行ldd /bin/sh, 会发现有个名字叫linux-vdso.so.1(老点的版本是linux-gate.so.1)的动态文件, 而系统中却找不到它, 它就是VDSO. 例如:

$ ldd /bin/sh
        linux-vdso.so.1 =>  (0x00007fff2f9ff000)
        libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f28d5b36000)
        /lib64/ld-linux-x86-64.so.2 (0x00007f28d5eca000)
linux-gate.so.1的地址是0xffffe000. 较新的内核提供了进程随机地址化功能, linux-vdso.so.1的地址每次cat /proc/self/maps都会变化, 想把它从/proc/self/mem中dd出来反汇编玩儿的同学请先:

echo "0" > /proc/sys/kernel/randomize_va_space
不光是快速系统调用, glibc现在也提供了VDSO的支持, open(), read(), write(), gettimeofday()都可以直接用VDSO中的实现, 使得这些调用更快, glibc更兼容, 内核新特性在不影响glibc的情况下也可以更快的部署. 实在是Nice!

ref:
1, http://lwn.net/Articles/18414/
2, http://www.ibm.com/developerworks/cn/linux/kernel/l-k26ncpu/index.html
```
