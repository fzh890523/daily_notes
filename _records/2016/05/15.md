# volatile 与 线程安全

### so网友回复

It's important to understand that there are two aspects to thread safety: (1) **execution control**, and (2) **memory visibility**. *The first has to do with controlling when code executes (including the order in which instructions are executed) and whether it can execute concurrently, and the second to do with when the effects in memory of what has been done are visible to other threads*. Because each CPU has several levels of cache between it and main memory, threads running on different CPUs or cores can see "memory" differently at any given moment in time because threads are permitted to obtain and work on private copies of main memory.

> ###by yonka
线程安全的两个方面：
* **执行控制**： 控制代码什么时候执行以及执行的顺序、是否能并发
* **内存可见性**： 与 已经完成（修改）的内容是否对其他线程可见 有关

> 原因:
* **重排序**： 编译器、CPU...
* **CPU缓存**： ...（如果vm实现...的话还有...，其实也就是线程local memory）

Using synchronized prevents any other thread from obtaining the monitor (or lock) for the same object, thereby preventing any and all code protected by synchronization on the same object from ever executing concurrently. Importantly, synchronization also creates a "happens-before" memory barrier, causing a memory visibility constraint such that anything that is done after some thread acquires a lock appears to another thread subsequently acquiring the same lock to have happened before that other thread acquired the lock. In practical terms, on current hardware, this typically causes flushing of the CPU caches when a monitor is acquired and writes to main memory when it is released, both of which are expensive (relatively speaking).



Using volatile, on the other hand, forces all accesses (read or write) to the volatile variable to occur to main memory, effectively keeping the volatile variable out of CPU caches. This can be useful for some actions where it is simply required that visibility of the variable be correct and order of accesses is not important. Using volatile also changes treatment of long and double to require accesses to them to be atomic; on some (older) hardware this might require locks, though not on modern 64 bit hardware. Under the new (JSR-133) memory model for Java 5+, the semantics of volatile have been strengthened to be almost as strong as synchronized with respect to memory visibility and instruction ordering (see http://www.cs.umd.edu/users/pugh/java/memoryModel/jsr-133-faq.html#volatile). For the purposes of visibility, each access to a volatile field acts like half a synchronization.

Under the new memory model, it is still true that volatile variables cannot be reordered with each other. The difference is that it is now no longer so easy to reorder normal field accesses around them. Writing to a volatile field has the same memory effect as a monitor release, and reading from a volatile field has the same memory effect as a monitor acquire. In effect, because the new memory model places stricter constraints on reordering of volatile field accesses with other field accesses, volatile or not, anything that was visible to thread A when it writes to volatile field f becomes visible to thread B when it reads f.

-- JSR 133 (Java Memory Model) FAQ
So, now both forms of memory barrier (under the current JMM) cause an instruction re-ordering barrier which prevents the compiler or run-time from re-ordering instructions across the barrier. In the old JMM, volatile did not prevent re-ordering. This can be important, because apart from memory barriers the only limitation imposed is that, for any particular thread, the net effect of the code is the same as it would be if the instructions were executed in precisely the order in which they appear in the source.

I just yesterday had some code where a shared but immutable object is recreated on the fly, with many other threads taking a reference to the object at a particular point in their execution cycle (at the start of handling a message) - volatile is perfect for that situation. I needed the other threads to begin using the recreated object as soon as it was published, but did not need the additional overhead of full synchronization and it's attendant contention and cache flushing.

Speaking to your read-update-write question, specifically. Consider the following unsafe code:

public void updateCounter() {
    if(counter==1000) { counter=0; }
    else              { counter++; }
    }
Now, with the updateCounter() method unsynchronized, two threads may enter it at the same time. Among the many permutations of what could happen, one is that thread1 does the test for counter==1000 and finds it true and is then suspended. Then thread2 does the same test and also sees it true and is suspended. Then thread1 resumes and sets counter to 0. Then thread2 resumes and again sets counter to 0 because it missed the update from thread1. This can also happen even if thread switching does not occur as I have described, but simply because two different cached copies of counter were present in two different CPU cores and the threads each ran on a separate core. For that matter, one thread could have counter at one value and the other could have counter at some entirely different value just because of caching.

What's important in this example is that the variable counter was read from main memory into cache, updated in cache and only written back to main memory at some indeterminate point later when a memory barrier occurred or when the cache memory was needed for something else. Making the counter volatile is insufficient for thread-safety of this code, because the test for the maximum and the assignments are discrete operations, including the increment which is a non-atomic read, increment, write, which at the machine level is something like:

MOV EAX,counter
INC EAX
MOV counter,EAX
Volatile variables are useful only when all operations performed on them are "atomic", such as my example where a reference to a fully formed object is only read or written (and, indeed, typically it's only written from a single point). Another example would be a volatile array reference backing a copy-on-write list, provided the array was only read by first taking a local copy of the reference to it.


### so网友回复

volatile is a field modifier, while synchronized modifies code blocks and methods. So we can specify three variations of a simple accessor using those two keywords:
     int i1;
     int geti1() {return i1;}

     volatile int i2;
     int geti2() {return i2;}

     int i3;
     synchronized int geti3() {return i3;}
geti1() accesses the value currently stored in i1 in the current thread. Threads can have local copies of variables, and the data does not have to be the same as the data held in other threads.In particular, another thread may have updated i1 in it's thread, but the value in the current thread could be different from that updated value. In fact Java has the idea of a "main" memory, and this is the memory that holds the current "correct" value for variables. Threads can have their own copy of data for variables, and the thread copy can be different from the "main" memory. So in fact, it is possible for the "main" memory to have a value of 1 for i1, for thread1 to have a value of 2 for i1 and for thread2 to have a value of 3 for i1 if thread1 and thread2 have both updated i1 but those updated value has not yet been propagated to "main" memory or other threads.

On the other hand, geti2() effectively accesses the value of i2 from "main" memory. A volatile variable is not allowed to have a local copy of a variable that is different from the value currently held in "main" memory. Effectively, a variable declared volatile must have it's data synchronized across all threads, so that whenever you access or update the variable in any thread, all other threads immediately see the same value. Generally volatile variables have a higher access and update overhead than "plain" variables. Generally threads are allowed to have their own copy of data is for better efficiency.

There are two differences between volitile and synchronized.

Firstly synchronized obtains and releases locks on monitors which can force only one thread at a time to execute a code block. That's the fairly well known aspect to synchronized. But synchronized also synchronizes memory. In fact synchronized synchronizes the whole of thread memory with "main" memory. So executing geti3() does the following:

The thread acquires the lock on the monitor for object this .
The thread memory flushes all its variables, i.e. it has all of its variables effectively read from "main" memory .
The code block is executed (in this case setting the return value to the current value of i3, which may have just been reset from "main" memory).
(Any changes to variables would normally now be written out to "main" memory, but for geti3() we have no changes.)
The thread releases the lock on the monitor for object this.
So where volatile only synchronizes the value of one variable between thread memory and "main" memory, synchronized synchronizes the value of all variables between thread memory and "main" memory, and locks and releases a monitor to boot. Clearly synchronized is likely to have more overhead than volatile.
http://javaexp.blogspot.com/2007/12/difference-between-volatile-and.html



# linux下几种系统调用的方式
从实现和使用的两个角度

## 实现
* 中断 int 0x80
* vsyscall （x86_64开始）

## 使用
* glibc提供的对应函数（read等）
> 最主要的使用方式，方便、规范。

  ```c
  #include <sys/types.h>
  #include <sys/stat.h>
  #include <errno.h>
  #include <stdio.h>

  int main()
  {
          int rc;

          rc = chmod("/etc/passwd", 0444);
          if (rc == -1)
                  fprintf(stderr, "chmod failed, errno = %d\n", errno);
          else
                  printf("chmod success!\n");
          return 0;
  }
  ```
* glibc提供的syscall函数
> 对前一种方式的补充，因为有些特殊系统调用glibc里可能没有

  ```c
  #include <stdio.h>
  #include <unistd.h>
  #include <sys/syscall.h>
  #include <errno.h>

  int main()
  {
          int rc;
          rc = syscall(SYS_chmod, "/etc/passwd", 0444);

          if (rc == -1)
                  fprintf(stderr, "chmod failed, errno = %d\n", errno);
          else
                  printf("chmod succeess!\n");
          return 0;
  }
  ```
* 自行ASM触发int 0x80

  ```c
  #include <stdio.h>
  #include <sys/types.h>
  #include <sys/syscall.h>
  #include <errno.h>

  int main()
  {
          long rc;
          char *file_name = "/etc/passwd";
          unsigned short mode = 0444;

          asm(
                  "int $0x80"
                  : "=a" (rc)
                  : "0" (SYS_chmod), "b" ((long)file_name), "c" ((long)mode)
          );

          if ((unsigned long)rc >= (unsigned long)-132) {
                  errno = -rc;
                  rc = -1;
          }

          if (rc == -1)
                  fprintf(stderr, "chmode failed, errno = %d\n", errno);
          else
                  printf("success!\n");

          return 0;
  }
  ```

**选择**
优先用glibc对应函数，其次glibc syscall函数，最次int 0x80。
因为从前面可以看到，系统调用可能有多种实现（架构、CPU厂商），而屏蔽这个差异的事情应该交给glibc。

# 总线： 地址总线、数据总线、控制总线

> 部分内容转自 [UNIX茶餐厅 - 总线 与 数据的字节对齐 ](http://blog.chinaunix.net/uid-22283027-id-3985411.html)

## 地址总线 - AB
1. CPU是通过地址总线来指定存储单元的。
2. 地址总线决定了cpu所能访问的最大内存空间的大小。eg: 10根地址线能访问的最大的内存为1024位二进制数据(1B)
3. 地址总线是地址线数量之和。

**地址总线宽度决定了可寻址空间大小**
同时，目前架构了少量地址给IO设备用，绝大多数地址用于内存寻址，所以基本上**决定了可用内存大小**（即使物理内存可以更大，但超出部分无法使用）

### AB宽度 vs DB宽度
**地址总线宽度不一定等于（一般大于或等于）数据总线宽度**
为了利用大内存，需要大寻址空间。
如 8086 DB宽度为16，AB宽度为20

一样
* 好处：
  * CPU可以一次寻址（一次性把地址数据发到AB）
* 坏处
  * DB宽度受限于CPU工艺，不那么容易提升，而AB宽度 == DB宽度的话限制了地址空间

不一样
* 好处：
  * 可以利用更大地址空间（AB宽度 > DB宽度）
* 坏处
  * 需要多次CPU操作才能完成一次寻址（可以利用其它技术来“折中”）


### 对齐问题
CPU通过地址总线来存取内存中的数据, 32位的CPU的地址总线宽度既为32位置, 标为A[0:31]. 在一个总线周期内, CPU从内存读/写32位.
但是`某些CPU只能在能够被4整除的地址进行内存访问`, 这是因为: `32位CPU不使用地址总线的A1和A2. (比如ARM, 它的A[0:1]用于字节选择, 用于逻辑控制, 而不和存储器相连, 存储器连接到A[2:31]`.)
访问内存的最小单位是字节(byte), A0和A1不使用, 那么对于地址来说, 最低两位是无效的, 所以它只能识别能被4整除的地址了. 在4字节中, 通过A0和A1确定某一个字节.

通常情况而言, 字节对齐只和数据总线相关. 但也会有一些CPU, 多少回和地址总线有些关系的. 比如ARM.
（顺便说一句，x86系cpu，从来都能支持非对齐地址访问，只是对非对齐地址访问，效率低而已。）

## 数据总线 - DB
1. 是CPU与内存或其他器件之间的数据传送的通道。
2. 数据总线的宽度决定了CPU和外界的数据传送速度。
3. 每条传输线一次只能传输1位二进制数据。eg: 8根数据线一次可传送一个8位二进制数据(即一个字节)。
4. 数据总线是数据线数量之和。

== `ALU宽度`，也即通常所说的**字长** （字长是按byte，所以32位CPU ALU宽度为4 && 字长为4，数据总线宽度也为4）

**ALU宽度受限于CPU工艺**

## 控制总线 - CB
1. CPU通过控制总线对外部器件进行控制。
2. 控制总线的宽度决定了CPU对外部器件的控制能力。
3. 控制总线是控制线数量之和。

> 一次操作通常是： 控制信号 + 地址信号

### 常见的控制信号
* 时钟
用来同步各种操作。
* 复位
初始化所有部件。
* 总线请求
表示部件需要获得的使用权。
* 总线允许
表示需要获得总线使用权的部件已获得了使用权。
* 中断请求
表示部件提出中断请求。
* 中断响应
表示中断请求已被接收。
* 存储器写
将数据总线上的数据写至存储器的指定地址单元内。
* 存储器读
将指定存储单元中的数据读到数据总线上。
* I/O读
从指定的I/O端口将数据读到数据总线上。
* I/O写
将数据总线上的数据输出到指定的I/O端口内。
* 传输响应
表示数据已被接收，或已将数据送至数据总线上。


# 虚拟化 - 全虚拟化 vs 半虚拟化的区别 && CPU的ring0、ring1
```
ring0是指CPU的运行级别，ring0是最高级别，ring1次之，ring2更次之……
拿Linux+x86来说，
操作系统（内核）的代码运行在最高运行级别ring0上，可以使用特权指令，控制中断、修改页表、访问设备等等。
应用程序的代码运行在最低运行级别上ring3上，不能做受控操作。如果要做，比如要访问磁盘，写文件，那就要通过执行系统调用（函数），执行系统调用的时候，CPU的运行级别会发生从ring3到ring0的切换，并跳转到系统调用对应的内核代码位置执行，这样内核就为你完成了设备访问，完成之后再从ring0返回ring3。这个过程也称作用户态和内核态的切换。

那么，虚拟化在这里就遇到了一个难题，因为宿主操作系统是工作在ring0的，客户操作系统就不能也在ring0了，但是它不知道这一点，以前执行什么指令，现在还是执行什么指令，那肯定不行啊，没权限啊，玩不转啊。所以这时候虚拟机管理程序（VMM）就要避免这件事情发生。
（VMM在ring0上，一般以驱动程序的形式体现，驱动程序都是工作在ring0上，否则驱动不了设备）
一般是这样做，客户操作系统执行特权指令时，会触发异常（CPU机制，没权限的指令，触发异常），然后VMM捕获这个异常，在异常里面做翻译，模拟，最后返回到客户操作系统内，客户操作系统认为自己的特权指令工作正常，继续运行。但是这个性能损耗，就非常的大，你想想原来，简单的一条指令，执行完，了事，现在却要通过复杂的异常处理过程。

这时候半虚拟化就来了，半虚拟化的思想就是，让客户操作系统知道自己是在虚拟机上跑的，工作在非ring0状态，那么它原先在物理机上执行的一些特权指令，就会修改成其他方式，这种方式是可以和VMM约定好的，这就相当于，我通过修改代码把操作系统移植到一种新的架构上来，就是定制化。所以像XEN这种半虚拟化技术，客户机操作系统都是有一个专门的定制内核版本，和x86、mips、arm这些内核版本等价。这样以来，就不会有捕获异常、翻译、模拟的过程了，性能损耗非常低。这就是XEN这种半虚拟化架构的优势。这也是为什么XEN只支持虚拟化Linux，无法虚拟化windows原因，微软不改代码啊。

可以后来，CPU厂商，开始支持虚拟化了，情况有发生变化，拿X86 CPU来说，引入了Intel-VT 技术，支持Intel-VT 的CPU，有VMX root operation 和 VMX non-root operation两种模式，两种模式都支持Ring 0 ~ Ring 3 这 4 个运行级别。这下好了，VMM可以运行在VMX root operation模式下，客户OS运行在VMX non-root operation模式下。也就说，硬件这层做了些区分，这样全虚拟化下，有些靠“捕获异常-翻译-模拟”的实现就不需要了。而且CPU厂商，支持虚拟化的力度越来越大，靠硬件辅助的全虚拟化技术的性能逐渐逼近半虚拟化，再加上全虚拟化不需要修改客户操作系统这一优势，全虚拟化技术应该是未来的发展趋势。

XEN是最典型的半虚拟化，不过现在XEN也支持硬件辅助的全虚拟化，大趋势，拗不过啊。。。
KVM、VMARE这些一直都是全虚拟化。
```
> 转自网络，没找着出处
