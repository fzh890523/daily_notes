
# Hive自定义分隔符InputFormat

作者: Michael 日期: 2014 年 2 月 24 日 发表评论 (1)查看评论
Hive默认创建的表字段分隔符为：\001(ctrl-A)，也可以通过 ROW FORMAT DELIMITED FIELDS TERMINATED BY 指定其他字符，但是该语法只支持单个字符，如果你的分隔符是多个字符，则需要你自定义InputFormat来实现，本文就以简单的示例演示多个字符作为分隔符的实现。

[一]、开发环境

Hadoop 2.2.0
Hive 0.12.0
Java1.6+
Mac OSX 10.9.1
[二]、示例

1、准备演示数据 mydemosplit.txt


michael|@^_^@|hadoop|@^_^@|http://www.micmiu.com/opensource/hadoop/hive-metastore-config/
michael|@^_^@|j2ee|@^_^@|http://www.micmiu.com/j2ee/hibernate/hibernate-jpa-demo/
michael|@^_^@|groovy|@^_^@|http://www.micmiu.com/lang/groovy/groovy-running-ways/
michael|@^_^@|sso|@^_^@|http://www.micmiu.com/enterprise-app/sso/sso-cas-ldap-auth/
michael|@^_^@|hadoop|@^_^@|http://www.micmiu.com/opensource/hadoop/hive-tutorial-ddl-dml/
michael|@^_^@|j2ee|@^_^@|http://www.micmiu.com/j2ee/spring/springmvc-binding-date/
michael|@^_^@|hadoop|@^_^@|http://www.micmiu.com/opensource/hadoop/hadoop2x-cluster-setup/

michael|@^_^@|hadoop|@^_^@|http://www.micmiu.com/opensource/hadoop/hive-metastore-config/
michael|@^_^@|j2ee|@^_^@|http://www.micmiu.com/j2ee/hibernate/hibernate-jpa-demo/
michael|@^_^@|groovy|@^_^@|http://www.micmiu.com/lang/groovy/groovy-running-ways/
michael|@^_^@|sso|@^_^@|http://www.micmiu.com/enterprise-app/sso/sso-cas-ldap-auth/
michael|@^_^@|hadoop|@^_^@|http://www.micmiu.com/opensource/hadoop/hive-tutorial-ddl-dml/
michael|@^_^@|j2ee|@^_^@|http://www.micmiu.com/j2ee/spring/springmvc-binding-date/
michael|@^_^@|hadoop|@^_^@|http://www.micmiu.com/opensource/hadoop/hadoop2x-cluster-setup/
分隔符为：“|@^_^@| ”

2、编码实现InputFormat

MyDemoInputFormat.java


package com.micmiu.hive;

import java.io.IOException;

import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.FileSplit;
import org.apache.hadoop.mapred.InputSplit;
import org.apache.hadoop.mapred.JobConf;
import org.apache.hadoop.mapred.LineRecordReader;
import org.apache.hadoop.mapred.RecordReader;
import org.apache.hadoop.mapred.Reporter;
import org.apache.hadoop.mapred.TextInputFormat;

/**
 * 
 * hive 自定义分隔符 比如：|@^_^@|
 * 
 * @author &lt;a href="http://www.micmiu.com"&gt;Michael&lt;/a&gt;
 * @create Feb 24, 2014 3:11:16 PM
 * @version 1.0
 */
public class MyDemoInputFormat extends TextInputFormat {

    @Override
    public RecordReader&lt;LongWritable, Text&gt; getRecordReader(
            InputSplit genericSplit, JobConf job, Reporter reporter)
            throws IOException {
        reporter.setStatus(genericSplit.toString());
        MyDemoRecordReader reader = new MyDemoRecordReader(
                new LineRecordReader(job, (FileSplit) genericSplit));
        return reader;
    }

    public static class MyDemoRecordReader implements
            RecordReader&lt;LongWritable, Text&gt; {

        LineRecordReader reader;
        Text text;

        public MyDemoRecordReader(LineRecordReader reader) {
            this.reader = reader;
            text = reader.createValue();
        }

        @Override
        public void close() throws IOException {
            reader.close();
        }

        @Override
        public LongWritable createKey() {
            return reader.createKey();
        }

        @Override
        public Text createValue() {
            return new Text();
        }

        @Override
        public long getPos() throws IOException {
            return reader.getPos();
        }

        @Override
        public float getProgress() throws IOException {
            return reader.getProgress();
        }

        @Override
        public boolean next(LongWritable key, Text value) throws IOException {
            while (reader.next(key, text)) {
                // michael|@^_^@|hadoop|@^_^@|http://www.micmiu.com/opensource/hadoop/hive-metastore-config/
                String strReplace = text.toString().toLowerCase()
                        .replaceAll("\\|@\\^_\\^@\\|", "\001");
                Text txtReplace = new Text();
                txtReplace.set(strReplace);
                value.set(txtReplace.getBytes(), 0, txtReplace.getLength());
                return true;

            }
            return false;
        }
    }
}

package com.micmiu.hive;
 
import java.io.IOException;
 
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.FileSplit;
import org.apache.hadoop.mapred.InputSplit;
import org.apache.hadoop.mapred.JobConf;
import org.apache.hadoop.mapred.LineRecordReader;
import org.apache.hadoop.mapred.RecordReader;
import org.apache.hadoop.mapred.Reporter;
import org.apache.hadoop.mapred.TextInputFormat;
 
/**
 * 
 * hive 自定义分隔符 比如：|@^_^@|
 * 
 * @author &lt;a href="http://www.micmiu.com"&gt;Michael&lt;/a&gt;
 * @create Feb 24, 2014 3:11:16 PM
 * @version 1.0
 */
public class MyDemoInputFormat extends TextInputFormat {
 
    @Override
    public RecordReader&lt;LongWritable, Text&gt; getRecordReader(
            InputSplit genericSplit, JobConf job, Reporter reporter)
            throws IOException {
        reporter.setStatus(genericSplit.toString());
        MyDemoRecordReader reader = new MyDemoRecordReader(
                new LineRecordReader(job, (FileSplit) genericSplit));
        return reader;
    }
 
    public static class MyDemoRecordReader implements
            RecordReader&lt;LongWritable, Text&gt; {
 
        LineRecordReader reader;
        Text text;
 
        public MyDemoRecordReader(LineRecordReader reader) {
            this.reader = reader;
            text = reader.createValue();
        }
 
        @Override
        public void close() throws IOException {
            reader.close();
        }
 
        @Override
        public LongWritable createKey() {
            return reader.createKey();
        }
 
        @Override
        public Text createValue() {
            return new Text();
        }
 
        @Override
        public long getPos() throws IOException {
            return reader.getPos();
        }
 
        @Override
        public float getProgress() throws IOException {
            return reader.getProgress();
        }
 
        @Override
        public boolean next(LongWritable key, Text value) throws IOException {
            while (reader.next(key, text)) {
                // michael|@^_^@|hadoop|@^_^@|http://www.micmiu.com/opensource/hadoop/hive-metastore-config/
                String strReplace = text.toString().toLowerCase()
                        .replaceAll("\\|@\\^_\\^@\\|", "\001");
                Text txtReplace = new Text();
                txtReplace.set(strReplace);
                value.set(txtReplace.getBytes(), 0, txtReplace.getLength());
                return true;
 
            }
            return false;
        }
    }
}
ps: 自定义实现接口InputFormat 、RecordReader，具体可以参考源码中得Base64TextInputFormat.java

编译打成jar包后，把该jar包copy一份到<HOME_HIVE>/lib/目录下，需要退出并重新进入Hive CLI模式。

3、建表和导入数据

用参数 STORED AS file_format 建表：


hive&gt; CREATE TABLE micmiu_blog(author STRING, category STRING, url STRING) STORED AS INPUTFORMAT 'com.micmiu.hive.MyDemoInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
hive&gt; desc micmiu_blog;
OK
author                  string                  None                
category                string                  None                
url                     string                  None                
Time taken: 0.05 seconds, Fetched: 3 row(s)

hive&gt; CREATE TABLE micmiu_blog(author STRING, category STRING, url STRING) STORED AS INPUTFORMAT 'com.micmiu.hive.MyDemoInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
hive&gt; desc micmiu_blog;
OK
author                  string                  None                
category                string                  None                
url                     string                  None                
Time taken: 0.05 seconds, Fetched: 3 row(s)
导入上面的数据文件,对比导入前后表中的数据：


hive&gt;select * from  micmiu_blog;                                           OK                                       
Time taken: 0.033 seconds
hive&gt; LOAD DATA LOCAL INPATH '/Users/micmiu/no_sync/testdata/hadoop/hive/mydemosplit.txt' OVERWRITE INTO TABLE micmiu_blog;
Copying data from file:/Users/micmiu/no_sync/testdata/hadoop/hive/mydemosplit.txt
Copying file: file:/Users/micmiu/no_sync/testdata/hadoop/hive/mydemosplit.txt
Loading data to table default.micmiu_blog
Table default.micmiu_blog stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 601, raw_data_size: 0]
OK
Time taken: 0.197 seconds
hive&gt; select * from  micmiu_blog;                                                 OK                                       
michael hadoop  http://www.micmiu.com/opensource/hadoop/hive-metastore-config/
michael j2ee    http://www.micmiu.com/j2ee/hibernate/hibernate-jpa-demo/
michael groovy  http://www.micmiu.com/lang/groovy/groovy-running-ways/
michael sso http://www.micmiu.com/enterprise-app/sso/sso-cas-ldap-auth/
michael hadoop  http://www.micmiu.com/opensource/hadoop/hive-tutorial-ddl-dml/
michael j2ee    http://www.micmiu.com/j2ee/spring/springmvc-binding-date/
michael hadoop  http://www.micmiu.com/opensource/hadoop/hadoop2x-cluster-setup/
Time taken: 0.053 seconds, Fetched: 7 row(s)
hive&gt;

hive&gt;select * from  micmiu_blog;                                           OK                                       
Time taken: 0.033 seconds
hive&gt; LOAD DATA LOCAL INPATH '/Users/micmiu/no_sync/testdata/hadoop/hive/mydemosplit.txt' OVERWRITE INTO TABLE micmiu_blog;
Copying data from file:/Users/micmiu/no_sync/testdata/hadoop/hive/mydemosplit.txt
Copying file: file:/Users/micmiu/no_sync/testdata/hadoop/hive/mydemosplit.txt
Loading data to table default.micmiu_blog
Table default.micmiu_blog stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 601, raw_data_size: 0]
OK
Time taken: 0.197 seconds
hive&gt; select * from  micmiu_blog;                                                 OK                                       
michael hadoop  http://www.micmiu.com/opensource/hadoop/hive-metastore-config/
michael j2ee    http://www.micmiu.com/j2ee/hibernate/hibernate-jpa-demo/
michael groovy  http://www.micmiu.com/lang/groovy/groovy-running-ways/
michael sso http://www.micmiu.com/enterprise-app/sso/sso-cas-ldap-auth/
michael hadoop  http://www.micmiu.com/opensource/hadoop/hive-tutorial-ddl-dml/
michael j2ee    http://www.micmiu.com/j2ee/spring/springmvc-binding-date/
michael hadoop  http://www.micmiu.com/opensource/hadoop/hadoop2x-cluster-setup/
Time taken: 0.053 seconds, Fetched: 7 row(s)
hive&gt;
从上面的执行过程可以看出已经实现了自定义字符串作为分隔符。

—————–  EOF @Michael Sun —————–

原创文章，转载请注明： 转载自micmiu – 软件开发+生活点滴[ http://www.micmiu.com/ ]

本文链接地址: http://www.micmiu.com/bigdata/hive/hive-inputformat-string/


# hive sql语法解读
ref: [hive sql语法解读](http://ilovejavaforever.iteye.com/blog/760731)


SQL数据结构MySQLHadoop 
    引言： 原本打算把所有hive相关的sql语法，全部写下来，但是写着写着，发现自己仅仅是在翻译hive的wiki，没有什么有价值的东西，就停止了。今天心情郁闷，又看到这篇一个月前写的学习笔记，防止它永远封存于硬盘。发上来，以资纪念。
 
 
                                              hive sql 学习笔记(1)
一、 创建表 
    在官方的wiki里，example是这样的：
Sql代码  收藏代码
CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name  
  [(col_name data_type [COMMENT col_comment], ...)]  
  [COMMENT table_comment]  
  [PARTITIONED BY (col_name data_type  
    [COMMENT col_comment], ...)]  
  [CLUSTERED BY (col_name, col_name, ...)  
  [SORTED BY (col_name [ASC|DESC], ...)]  
  INTO num_buckets BUCKETS]  
  [ROW FORMAT row_format]  
  [STORED AS file_format]  
  [LOCATION hdfs_path]  
 
    [ROW FORMAT DELIMITED]关键字，是用来设置创建的表在加载数据的时候，支持的列分隔符；
[STORED AS file_format]关键字是用来设置加载数据的数据类型。Hive本身支持的文件格式只有：Text File，Sequence File。如果文件数据是纯文本，可以使用 [STORED AS TEXTFILE]。如果数据需要压缩，使用 [STORED AS SEQUENCE] 。通常情况，只要不需要保存序列化的对象，我们默认采用[STORED AS TEXTFILE]。
 
    那么我们创建一张普通的hive表，hive sql就如下：
Sql代码  收藏代码
CREATE TABLE test_1(id INT, name STRING, city STRING) SORTED BY TEXTFILE ROW FORMAT DELIMITED‘\t’  
 
    其中，hive支持的字段类型，并不多，可以简单的理解为数字类型和字符串类型，详细列表如下：
Sql代码  收藏代码
TINYINT  
SMALLINT  
INT  
BIGINT  
BOOLEAN  
FLOAT  
DOUBLE  
STRING  
 
    Hive的表，与普通关系型数据库，如mysql在表上有很大的区别，所有hive的表都是一个文件，它是基于Hadoop的文件系统来做的。
    hive总体来说可以总结为三种不同类型的表。

1. 普通表

    普通表的创建，如上所说，不讲了。其中，一个表，就对应一个表名对应的文件。
 
2. 外部表

    EXTERNAL 关键字可以让用户创建一个外部表，在建表的同时指定一个指向实际数据的路径（LOCATION），Hive 创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径，不对数据的位置做任何改变。在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。具体sql如下：
Sql代码  收藏代码
CREATE EXTERNAL TABLE test_1(id INT, name STRING, city STRING) SORTED BY TEXTFILE ROW FORMAT DELIMITED‘\t’ LOCATION ‘hdfs://../../..’  
 
3. 分区表

    有分区的表可以在创建的时候使用 PARTITIONED BY 语句。一个表可以拥有一个或者多个分区，每一个分区单独存在一个目录下。而且，表和分区都可以对某个列进行 CLUSTERED BY 操作，将若干个列放入一个桶（bucket）中。也可以利用SORT BY 对数据进行排序。这样可以为特定应用提高性能。具体SQL如下：
Sql代码  收藏代码
CREATE TABLE test_1(id INT, name STRING, city STRING) PARTITIONED BY (pt STRING) SORTED BY TEXTFILE ROW FORMAT DELIMITED‘\t’   
    Hive的排序，因为底层实现的关系，比较不同于普通排序，这里先不讲。
 
     桶的概念，主要是为性能考虑，可以理解为对分区内列，进行再次划分，提高性能。在底层，一个桶其实是一个文件。如果桶划分过多，会导致文件数量暴增，一旦达到系统文件数量的上限，就杯具了。哪种是最优数量，这个哥也不知道。
 
    分区表实际是一个文件夹，表名即文件夹名。每个分区，实际是表名这个文件夹下面的不同文件。分区可以根据时间、地点等等进行划分。比如，每天一个分区，等于每天存每天的数据；或者每个城市，存放每个城市的数据。每次查询数据的时候，只要写下类似 where pt=2010_08_23这样的条件即可查询指定时间得数据。
 
    总体而言，普通表，类似mysql的表结构，外部表的意义更多是指数据的路径映射。分区表，是最难以理解，也是最hive最大的优势。之后会专门针对分区表进行讲解。
 
二、 加载数据

    Hive不支持一条一条的用insert语句进行插入操作，也不支持update的操作。数据是以load的方式，加载到建立好的表中。数据一旦导入，则不可修改。要么drop掉整个表，要么建立新的表，导入新的数据。
官方指导为：
Sql代码  收藏代码
LOAD DATA [LOCAL] INPATH 'filepath' [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)]  
 
    Hive在数据load这块，大方向分为两种方式，load文件或者查询一张表，或者将某张表里的额查询结果插入指定表。
如果划分更细一点个人归纳总结为4种不同的方式的load：
 
1. Load data到指定的表

    直接将file，加载到指定的表，其中，表可以是普通表或者分区表。具体sql如下：
Sql代码  收藏代码
LOAD DATA LOCAL INPATH '/home/admin/test/test.txt' OVERWRITE INTO TABLE test_1  
     
    关键字[OVERWRITE]意思是是覆盖原表里的数据，不写则不会覆盖。
    关键字[LOCAL]是指你加载文件的来源为本地文件，不写则为hdfs的文件。
    其中
     ‘home/admin/test/test.txt’是相对路径
     ‘/home/admin/test/test.txt’为据对路径
 
2. load到指定表的分区

    直接将file，加载到指定表的指定分区。表本身必须是分区表，如果是普通表，导入会成功，但是数据实际不会被导入。具体sql如下：
Sql代码  收藏代码
LOAD DATA LOCAL INPATH '/home/admin/test/test.txt' OVERWRITE INTO TABLE test_1 PARTITION（pt=’xxxx）  
 
    load数据，hive支持文件夹的方式，将文件夹内的所有文件，都load到指定表中。Hdfs会将文件系统内的某文件夹路径内的文件，分散到不同的实际物理地址中。这样，在数据量很大的时候，hive支持读取多个文件载入，而不需要限定在唯一的文件中。
    
3. insert+select

    这个是完全不同于文件操作的数据导入方式。官方指导为：
Sql代码  收藏代码
Standard syntax:  
INSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement1 FROM from_statement   
  
Hive extension (multiple inserts):  
FROM from_statement  
INSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement1  
[INSERT OVERWRITE TABLE tablename2 [PARTITION ...] select_statement2] ...  
  
Hive extension (dynamic partition inserts):  
INSERT OVERWRITE TABLE tablename PARTITION (partcol1[=val1], partcol2[=val2] ...) select_statement FROM from_statement  
 
    这个的用法，和上面两种直接操作file的方式，截然不同。从sql语句本身理解，就是把查询到的数据，直接导入另外一张表。这个暂时不仔细分析，之后查询章节，再细讲。

4. alter 表，对分区操作

    在对表结构进行修改的时候，我们可以增加一个新的分区，在增加新分区的同时，将数据直接load到新的分区当中。
Sql代码  收藏代码
ALTER TABLE table_name ADD  
  partition_spec [ LOCATION 'location1' ]  
  partition_spec [ LOCATION 'location2' ] ...  
 
三、 查询
   ……


# 系列文章摘录： 写给大数据开发初学者的话

ref： [写给大数据开发初学者的话](http://lxw1234.com/archives/2016/11/779.htm)

local一份在 [写给大数据开发初学者的话](resource/写给大数据开发初学者的话) 下。

