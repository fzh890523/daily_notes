
# ncftp直接拉整个目录（递归）

```
to get/download FTP directory recursively
ncftpget -R -v -u 'ftpuser' -p 'ftppassword' ftp.yourdomain.org /local/backup/ /pub/dir
-R : Copy all subdirectories and files (recursive)
-v : Verbose i.e. display download activity and progess
-u 'ftpuser' : FTP server username, if skipped ncftpget will try anonymous username
-p 'ftppassword' : FTP password
ftp.yourdomain.org : FTP server name/address
/local/backup : Download everything to this directory
/pub/dir : Remote ftp directory you wish to copy
If you get an error which read as follows:
  tar: End of archive volume 1 reached
  tar: Sorry, unable to determine archive format.
  Could not read directory listing data: Connection reset by peer
Then add –T option to ncftpget command:
ncftpget -T –R –v –u 'ftpuser' -p 'ftppassword' ftp.yourdomain.org /local/backup/ /pub/dir
-T : Do not try to use TAR mode with Recursive mode
```

也可以在ncftp shell中 使用参数，如`get -R`

```
ncftp / > get -R monitor-log/releases/online/20161123/flume/
flume/gateway-0.0.1-SNAPSHOT.jar:                       13.99 kB   14.83 MB/s  
flume/netty-all-5.0.0.Alpha2.jar:                        2.56 MB  101.82 MB/s 
```


# zk查看kafka consumer的partition分布

> ref: [In Zookeeper client how to look up offset for a group id / topic?](http://stackoverflow.com/questions/27845999/in-zookeeper-client-how-to-look-up-offset-for-a-group-id-topic)

```
Take a look at this document:

https://cwiki.apache.org/confluence/display/KAFKA/Kafka+data+structures+in+Zookeeper

So, the offset will be stored in Zookeeper at:

/[clusterBasePath]/consumers/[groupId]/offsets/[topic]/[partitionId] -> long (offset)

If that znode is empty, it might mean that your consumer group hasn't owned the partitions yet (decided on a mapping from partitions to consumers).

To see the current partition owners in the consumer group, take a look at:

/[clusterBasePath]/consumers/[groupId]/owners/[topic]/[partitionId] -> string (consumerId)

From personal experience, the most common cause of this is that your your consumer group is having trouble owning partitions due to timeouts. On your consumer config, you might want to try increasing rebalance.max.retries to something like 50 (or higher) and rebalance.backoff.ms to something like 5000. Also, check your Zookeeper session timeouts and increase those if necessary.

Depending on what consumer you're using (are you using consumer groups at all?), there's also a chance that you're just not committing your offsets to Zookeeper (this is ok if you don't care too much about fault tolerance). In that case, you won't be able to find the offsets in Zookeeper and will need to get them from your consumer directly.
```

在`/[clusterBasePath]/consumers/[groupId]/offsets/[topic]/[partitionId] -> long (offset)`下存该partition的offset，如：
```
[zk: 172.18.5.221:2181(CONNECTED) 6] get /kafka/consumers/dapper/offsets/dapper_span/

0   1   2   3   4   5   6   7   8
[zk: 172.18.5.221:2181(CONNECTED) 6] get /kafka/consumers/dapper/offsets/dapper_span/0
36217283
cZxid = 0x1000001ca
ctime = Mon Nov 07 15:00:35 CST 2016
mZxid = 0x1008f36d2
mtime = Thu Nov 24 09:56:32 CST 2016
pZxid = 0x1000001ca
cversion = 0
dataVersion = 1028038
aclVersion = 0
ephemeralOwner = 0x0
dataLength = 8
numChildren = 0
```

在`/[clusterBasePath]/consumers/[groupId]/owners/[topic]/[partitionId] -> string (consumerId)`下面存该partition的owner（consumer），如：
```
[zk: 172.18.5.221:2181(CONNECTED) 7] get /kafka/consumers/dapper/owners/dapper_span/0 
dapper_localhost-1479715234755-a2e7372e-0
cZxid = 0x1008f25a0
ctime = Thu Nov 24 09:45:42 CST 2016
mZxid = 0x1008f25a0
mtime = Thu Nov 24 09:45:42 CST 2016
pZxid = 0x1008f25a0
cversion = 0
dataVersion = 0
aclVersion = 0
ephemeralOwner = 0xde5876b9bf3a003a
dataLength = 41
numChildren = 0
[zk: 172.18.5.221:2181(CONNECTED) 8] get /kafka/consumers/dapper/owners/dapper_span/1
dapper_localhost-1479715234755-a2e7372e-1
cZxid = 0x1008f25a2
ctime = Thu Nov 24 09:45:42 CST 2016
mZxid = 0x1008f25a2
mtime = Thu Nov 24 09:45:42 CST 2016
pZxid = 0x1008f25a2
cversion = 0
dataVersion = 0
aclVersion = 0
ephemeralOwner = 0xde5876b9bf3a003a
dataLength = 41
numChildren = 0
...
[zk: 172.18.5.221:2181(CONNECTED) 15] get /kafka/consumers/dapper/owners/dapper_span/8
dapper_localhost-1479951573075-69b4c0fd-0
cZxid = 0x1008f25b1
ctime = Thu Nov 24 09:45:43 CST 2016
mZxid = 0x1008f25b1
mtime = Thu Nov 24 09:45:43 CST 2016
pZxid = 0x1008f25b1
cversion = 0
dataVersion = 0
aclVersion = 0
ephemeralOwner = 0xdf57d614059704fb
dataLength = 41
numChildren = 0
```
注意，这里的`dapper_localhost-1479951573075-69b4c0fd-0`命名方式应该是kafka high-level api里的实现，最后面的数字应该是表示该consumer处理的第几个分区而前面的部分标识某个consumer，只对该consumer有意义，这样就使得在zk上，所有partition的owner从字符串层面来看都是不同的。


# udp datagram / unix domain datagram 最大包长

ref：
* [UDP IP Fragmentation and MTU](http://stackoverflow.com/questions/3712151/udp-ip-fragmentation-and-mtu)
* [What's the practical limit on the size of single packet transmitted over domain socket?](http://stackoverflow.com/questions/21856517/whats-the-practical-limit-on-the-size-of-single-packet-transmitted-over-domain)


```
Implementations of the IP protocol are not required to be capable of handling arbitrarily large packets. In theory, the maximum possible IP packet size is 65,535 octets, but the standard only requires that implementations support at least 576 octets.

It would appear that your host's implementation supports a maximum size much greater than 576, but still significantly smaller than the maximum theoretical size of 65,535. (I don't think the switch should be a problem, because it shouldn't need to do any defragmentation -- it's not even operating at the IP layer).

The IP standard further recommends that hosts not send packets larger than 576 bytes, unless they are certain that the receiving host can handle the larger packet size. You should maybe consider whether or not it would be better for your program to send a smaller packet size. 24,529 seems awfully large to me. I think there may be a possibility that a lot of hosts won't handle packets that large.

Note that these packet size limits are entirely separate from MTU (the maximum frame size supported by the data link layer protocol).
```

```
I found the following which may be of interest:

Determine the maximum size of a UDP datagram packet on Linux
Set the DF bit in the IP header and send continually larger packets to determine at what point a packet is fragmented as per Path MTU Discovery. Packet fragmentation should then result in a ICMP type 3 packet with code 4 indicating that the packet was too large to be sent without being fragmented.
Dan's answer is useful but note that after headers you're really limited to 65507 bytes.
```

```
There are a number of factors which will determine the maximum of size of a packet that can be sent on a Unix socket:

The wmem_max socket send buffer maximum size kernel setting, which determines the maximum size of the send buffer that can be set using setsockopt (SO_SNDBUF). The current setting can be read from /proc/sys/net/core/wmem_max and can be set using sysctl net.core.wmem_max=VALUE (add the setting to /etc/sysctl.conf to make the change persistent across reboots). Note this setting applies to all sockets and socket protocols, not just to Unix sockets.
If multiple packets are sent to a Unix socket (using SOCK_DATAGRAM), then the maximum amount of data which can be sent without blocking depends on both the size of the socket send buffer (see above) and the maximum number of unread packets on the Unix socket (kernel parameter net.unix.max_dgram_qlen).
Finally, a packet (SOCK_DATAGRAM) requires contiguous memory (as per What is the max size of AF_UNIX datagram message that can be sent in linux?). How much contiguous memory is available in the kernel will depend on many factors (e.g. the I/O load on the system, etc...).
So to maximize the performance on your application, you need a large socket buffer size (to minimize the user/kernel space context switches due to socket write system calls) and a large Unix socket queue (to decouple the producer and consumer as much as possible). However, the product of the socket send buffer size and queue length must not be so large as to cause the kernel to run out of contiguous memory areas (causing write failures).

The actual figures will depend on your system configuration and usage. You will need to determine the limits by testing... start say with wmem_max at 256Kb and max_dgram_qlen at 32 and keep doubling wmem_max until you notice things start breaking. You will need to adjust max_dgram_qlen to balance the activity of the producer and consumer to a certain extent (although if the producer is much faster or much slower than the consumer, the queue size won't have much affect).

Note your producer will have to specifically setup the socket send buffer size to wmem_max bytes with a call to setsockopt (SO_SNDBUF) and will have to split data into wmem_max byte chunks (and the consumer will have to reassemble them).

Best guess: the practical limits will be around wmem_max ~8Mb and unix_dgram_qlen ~32.
```

## 结论

根据ref，大约如下：
* **udp** 65507，但一般最好在以太网链路/ip的基础上考虑，大约14xx
* **unix** 10M？ = = 

