
# 了解一下ngx_slowfs_cache，理解和proxy cache的区别

ref：

*   [nginx本地缓存模块ngx_slowfs_cache](http://www.ttlsa.com/nginx/local-nginx-cache-module-ngx_slowfs_cache/)
*   [使用ngx_slowfs_cache模块构建本地缓存](http://www.wenzizone.cn/2010/09/21/%E4%BD%BF%E7%94%A8ngx_slowfs_cache%E6%A8%A1%E5%9D%97%E6%9E%84%E5%BB%BA%E6%9C%AC%E5%9C%B0%E7%BC%93%E5%AD%98.html)
*   [github: FRiCKLE/ngx_slowfs_cache](https://github.com/FRiCKLE/ngx_slowfs_cache)

简直言之就是proxy cache本身需要在proxy场景下使用，而如果是本地文件，比如希望对本地文件做一个cache（ssd之于sata）的话，就不行了。
slowfs_cache抄了一个proxy_cache的“本地”版本。 = =

```
nginx本地缓存模块ngx_slowfs_cache

nginx proxy反向代理本身就支持缓存的，但是如果没有使用到nginx反向代理的话，就需要使用ngx_slowfs_cache模块来实现本地站点静态文件缓存，同时还为低速的存储设备创建快速缓存。
1. 安装ngx_slowfs_cache和ngx_cache_purge模块

# wget http://labs.frickle.com/files/ngx_slowfs_cache-1.10.tar.gz
# wget http://labs.frickle.com/files/ngx_cache_purge-2.1.tar.gz
# tar zxvf ngx_slowfs_cache-1.10.tar.gz
# tar zxvf ngx_cache_purge-2.1.tar.gz
# cd nginx-1.2.5
# ./configure --prefix=/usr/local/nginx-1.2.5 \
--with-http_stub_status_module --with-http_realip_module \
--add-module=../ngx_slowfs_cache-1.10 \
--add-module=../ngx_cache_purge-2.1
# make
# make install

# wget http://labs.frickle.com/files/ngx_slowfs_cache-1.10.tar.gz
# wget http://labs.frickle.com/files/ngx_cache_purge-2.1.tar.gz
# tar zxvf ngx_slowfs_cache-1.10.tar.gz
# tar zxvf ngx_cache_purge-2.1.tar.gz
# cd nginx-1.2.5
# ./configure --prefix=/usr/local/nginx-1.2.5 \
--with-http_stub_status_module --with-http_realip_module \
--add-module=../ngx_slowfs_cache-1.10 \
--add-module=../ngx_cache_purge-2.1
# make
# make install
2. 使用

http {
    slowfs_cache_path /data/cache/proxy_cache_dir  levels=1:2   keys_zone=fastcache:4096m inactive=1d max_size=20g;
    slowfs_temp_path  /data/cache/proxy_temp_dir 1 2;

    server {
        location ~ /purge(/.*) {
            allow               127.0.0.1;
            allow               10.0.0.0/8;
            deny                all;
            slowfs_cache_purge  fastcache $1;
        }

        location ~* \.(gif|jpg|jpeg|css|js|bmp|png)$ {
            slowfs_cache        fastcache;
            slowfs_cache_key    $uri;
            slowfs_cache_valid  1d;
            add_header X-Cache '$slowfs_cache_status from $host';
            expires  max;
        }

    }
}

http {
    slowfs_cache_path /data/cache/proxy_cache_dir  levels=1:2   keys_zone=fastcache:4096m inactive=1d max_size=20g;
    slowfs_temp_path  /data/cache/proxy_temp_dir 1 2;
 
    server {
     location ~ /purge(/.*) {
            allow               127.0.0.1;
            allow               10.0.0.0/8;
            deny                all;
            slowfs_cache_purge  fastcache $1;
        }
 
        location ~* \.(gif|jpg|jpeg|css|js|bmp|png)$ {
            slowfs_cache        fastcache;
            slowfs_cache_key    $uri;
            slowfs_cache_valid  1d;
            add_header X-Cache '$slowfs_cache_status from $host';
            expires  max;
        }
 
    }
}
说明：slowfs_cache_path和slowfs_temp_path需要在同一分区。 slowfs_cache_path指定缓存文件的目录级数，缓存区名称为fastcache，内存缓存空间为4096m，1天没有被访问的内容自动清除，硬盘缓存空间为20g。slowfs_cache_purge为清除缓存。
要注意location执行顺序。nginx purge更新缓存404错误 一例参见http://www.ttlsa.com/html/1084.html
3. 模块指令说明
slowfs_cache
语法：slowfs_cache zone_name
默认值：none
配置段：http, server, location
定义使用的缓存区。要与slowfs_cache_path指令定义的一致。
slowfs_cache_key
语法: slowfs_cache_key key
默认值: none
配置段: http, server, location
设置缓存的键
slowfs_cache_purge
语法: slowfs_cache_purge zone_name key
默认值: none
配置段: location
根据指定的key从缓存中清除也存在的缓存
slowfs_cache_path
语法: slowfs_cache_path path [levels] keys_zone=zone_name:zone_size [inactive] [max_size]
默认值: none
配置段: http
设置缓存区和缓存结构
slowfs_temp_path
语法: slowfs_temp_path path [level1] [level2] [level3]
默认值: /tmp 1 2
配置段: http
设置临时区。文件在移到缓存区时的临时存储地。
slowfs_cache_min_uses
语法: slowfs_cache_min_uses number
默认值: 1
配置段: http, server, location
设置文件被访问多少次后复制到缓存
slowfs_cache_valid
语法: slowfs_cache_valid [reply_code] time
默认值: none
配置段: http, server, location
设置缓存时间
slowfs_big_file_size
语法: slowfs_big_file_size size
默认值: 128k
配置段: http, server, location
设置大文件阀值，避免服务中断
$slowfs_cache_status变量：
表示缓存文件的可用性，值有MISS, HIT, EXPIRED
4. 测试
nginx
5. 清缓存
nginx
对于文件可以这么来做缓存。那么对于要经过php处理过的该怎么缓存呢？ 请看下篇。
```

# 归总一下tcp nodelay的特性，尤其是对SYNC的ack是否也...

ref：

*   [Why would a server not send a SYN/ACK packet in response to a SYN packet](http://serverfault.com/questions/235965/why-would-a-server-not-send-a-syn-ack-packet-in-response-to-a-syn-packet)
*   [tcp-delayed-ack-combined-with-nagle-algorithm-can-badly-impact-communication-performance/](https://blogs.technet.microsoft.com/nettracer/2013/01/05/tcp-delayed-ack-combined-with-nagle-algorithm-can-badly-impact-communication-performance/)
*   [TCP delay ack机制和实现](http://blog.chinaunix.net/uid-28387257-id-3658980.html)
*   [神秘的40毫秒延迟与 TCP_NODELAY](http://jerrypeng.me/2013/08/mythical-40ms-delay-and-tcp-nodelay/) or [神秘的40毫秒延迟与 TCP_NODELAY - local](resource/神秘的40毫秒延迟与 TCP_NODELAY - Jerry's Blog.html)
*   [记一次丢包网络故障](http://huoding.com/2013/02/26/233)

*   [TCP_NODELAY 和 TCP_NOPUSH](http://xiaomaimai.blog.51cto.com/1182965/1557773) or [TCP_NODELAY 和 TCP_NOPUSH - local](resource/TCP_NODELAY 和 TCP_NOPUSH - 麦麦的运维之路 - 51CTO技术博客.html)

*   [when-should-i-use-tcp-nodelay-and-when-tcp-cork](http://stackoverflow.com/questions/3761276/when-should-i-use-tcp-nodelay-and-when-tcp-cork)
*   [TCP 的那些事儿（下）](http://coolshell.cn/articles/11609.html)

*   [nginx-tuning-for-best-performance-255](http://www.codestance.com/tutorials-archive/nginx-tuning-for-best-performance-255)

*   [nginx-tuning](http://www.revsys.com/12days/nginx-tuning/)
*   [NGINX Tuning For Best Performance](https://gist.github.com/denji/8359866)


## 跟进

### 背景

####  Nagle‘s Algorithm 
```
Nagle’s Algorithm 是为了提高带宽利用率设计的算法，其做法是合并小的TCP 包为一个，避免了过多的小报文的 TCP 头所浪费的带宽。如果开启了这个算法 （默认），则协议栈会累积数据直到以下两个条件之一满足的时候才真正发送出 去：
1. 积累的数据量到达最大的 TCP Segment Size
2. 收到了一个 Ack
```

```python
if there is new data to send
  if the window size >= MSS and available data is >= MSS
    send complete MSS segment now
  else
    if there is unconfirmed data still in the pipe
      enqueue data in the buffer until an acknowledge is received
    else
      send data immediately
    end if
  end if
end if
```

积累size不足 && 有还没被ACK的数据 ===> buf该数据

```
另外，Nagle算法默认是打开的，所以，对于一些需要小包场景的程序——比如像telnet或ssh这样的交互性比较强的程序，你需要关闭这个算法。你可以在Socket设置TCP_NODELAY选项来关闭这个算法（关闭Nagle算法没有全局参数，需要根据每个应用自己的特点来关闭）
```

##### nagle vs TCP_CORK

```
另外，网上有些文章说TCP_CORK的socket option是也关闭Nagle算法，这不对。TCP_CORK其实是更新激进的Nagle算汉，完全禁止小包发送，而Nagle算法没有禁止小包发送，只是禁止了大量的小包发送。最好不要两个选项都设置。
```

```
TCP_CORK选项的功能类似于在发送数据管道出口处插入一个“塞子”，使得发送数据全部被阻塞，直到取消TCP_CORK选项（即拔去塞子）或被阻塞数据长度已超过MSS才将其发送出去。

选项TCP_NODELAY是禁用Nagle算法，即数据包立即发送出去，而选项TCP_CORK与此相反，可以认为它是Nagle算法的进一步增强，即阻塞数据包发送，
具体点说就是：TCP_CORK选项的功能类似于在发送数据管道出口处插入一个“塞子”，使得发送数据全部被阻塞，
直到取消TCP_CORK选项（即拔去塞子）或被阻塞数据长度已超过MSS才将其发送出去。
举个对比示例，比如收到接收端的ACK确认后，Nagle算法可以让当前待发送数据包发送出去，即便它的当前长度仍然不够一个MSS，
但选项TCP_CORK则会要求继续等待，这在前面的tcp_nagle_check()函数分析时已提到这一点，即如果包数据长度小于当前MSS &&（（加塞 || …）|| …），那么缓存数据而不立即发送：

在TCP_NODELAY模式下，假设有3个小包要发送，第一个小包发出后，接下来的小包需要等待之前的小包被ack，在这期间小包会合并，直到接收到之前包的ack后才会发生；
而在TCP_CORK模式下，第一个小包都不会发生成功，因为包太小，发生管道被阻塞，同一目的地的小包彼此合并后组成一个大于mss的包后，才会被发生

TCP_CORK选项“堵塞”特性的最终目的无法是为了提高网络利用率，既然反正是要发一个数据包（零窗口探测包），
如果有实际数据等待发送，那么干脆就直接发送一个负载等待发送数据的数据包岂不是更好？

我们已经知道，TCP_CORK选项的作用主要是阻塞小数据发送，所以在nginx内的用处就在对响应头的发送处理上。
一般而言，处理一个客户端请求之后的响应数据包括有响应头和响应体两部分，那么利用TCP_CORK选项就能让这两部分数据一起发送：

假设我们需要等到数据量达到最大时才通过网络一次发送全部数据，这种数据传输方式有益于大量数据的通信性能，典型的应用就是文件服务器。
应用Nagle算法在这种情况下就会产生问题。因为TCP_NODELAY在发生小包时不再等待之前的包有没有ack，网络中会存在较多的小包，但这会影响网络的传输能力；
但是，如果你正在发送大量数据，你可以设置TCP_CORK选项禁用Nagle化，其方式正好同 TCP_NODELAY相反（TCP_CORK 和 TCP_NODELAY 是互相排斥的）。

下面就让我们仔细分析下其工作原理。 
假设应用程序使用sendfile()函数来转移大量数据。应用协议通常要求发送某些信息来预先解释数据，这些信息其实就是报头内容。
典型情况下报头很小，而且套接字上设置了TCP_NODELAY。有报头的包将被立即传输，在某些情况下（取决于内部的包计数器），因为这个包成功地被对方收到后需要请求对方确认。
这样，大量数据的传输就会被推迟而且产生了不必要的网络流量交换。
 
但是，如果我们在套接字上设置了TCP_CORK（可以比喻为在管道上插入“塞子”）选项，具有报头的包就会填补大量的数据，所有的数据都根据大小自动地通过包传输出去。
当数据传输完成时，最好取消TCP_CORK 选项设置给连接“拔去塞子”以便任一部分的帧都能发送出去。这同“塞住”网络连接同等重要。 

总而言之，如果你肯定能一起发送多个数据集合（例如HTTP响应的头和正文），那么我们建议你设置TCP_CORK选项，这样在这些数据之间不存在延迟。
能极大地有益于WWW、FTP以及文件服务器的性能，同时也简化了你的工作。
```

####  TCP Delayed Acknoledgement 
```
TCP Delayed Acknoledgement 也是为了类似的目的被设计出来的，它的作用就 是延迟 Ack 包的发送，使得协议栈有机会合并多个 Ack，提高网络性能。
```

> ref: [TCP delay ack机制和实现 - local](http://blog.chinaunix.net/uid-28387257-id-3658980.html) or [TCP delay ack机制和实现  - local](resource/TCP delay ack机制和实现-henrystark-ChinaUnix博客.html)

大约流程如下：
1. 检查是否需要立即发送，有以下之一者立即发送，否则进入2的延迟发送模式
    * 接收窗口中有>1个段没被ack
    * 在快速确认模式。 以下情况会进入该模式
        - synsent状态处理
        - 发送dupack
        - 接收到窗口之外的数据段
        - 接收到ECN标志段
    * 有乱序段
2. 延迟发送，通过定时器（默认40ms）控制，如果期间一直没达到条件的话，会在定时器到期后发送

### 分析

```
如果一个 TCP 连接的一端启用了 Nagle‘s Algorithm，而另一端启用了 TCP Delayed Ack，而发送的数据包又比较小，则可能会出现这样的情况：发送端在等 待接收端对上一个packet 的 Ack 才发送当前的 packet，而接收端则正好延迟了 此 Ack 的发送，那么这个正要被发送的 packet 就会同样被延迟。当然 Delayed Ack 是有个超时机制的，而默认的超时正好就是 40ms。
现代的 TCP/IP 协议栈实现，默认几乎都启用了这两个功能，你可能会想，按我 上面的说法，当协议报文很小的时候，岂不每次都会触发这个延迟问题？事实不 是那样的。仅当协议的交互是发送端连续发送两个 packet，然后立刻 read 的 时候才会出现问题。
```

#### 典型场景： http小body请求
A: nagle
B: delayed ack

A -> http header -> B  // a side no non-acked-data, thus send data immediately
--- no ack, delayed by B
A -> http body  // a side has non-acked-data, buf this data and wait
--- buffered by nagle of A side
A -> read response
A <- ack(http header) <- B-tcp-stack  // as ack-delay-timer timeout
A-tcp-stack -> http body -> B

重点在于： 
* request body太小不能“靠自己”触发flush
* 场景为典型的write-write-read
    - 如果是write-read-write-read的话，表示第一个write已经写完本次（请求）数据，那么接收方应该返回response了，这样会把req的ack一起带回来。 所以第三个write（新的“轮回”）就不受影响了。 当然这里指的是典型场景，包含一些背设： 既然都read了那么该write的应该已经write完了
    - 而write-write-write，其实还好，因为多个write，不至于积累不到MSS吧。 但也有极端情况： 多个小包。 这样的话和前一个类似了。

TODO 测一下该场景...

#### 遇到过的场景： 周期性的sync后等sync/ack超时

按照前面的分析，sync是第一个包不被buf，而sync/ack本身不（只）是ack也不存在delay ack的问题，至于回复的syn本身也是第一个包更不存在buf的问题了，so... ？

TODO 测一下
http://serverfault.com/questions/235965/why-would-a-server-not-send-a-syn-ack-packet-in-response-to-a-syn-packet

# 测试网络是否丢包的方式： ping flood
```
如何判断网络是否存在丢包呢？非常简单，通过常用的「ping」命令即可：

shell> ping -f <IP>
关于其中的「-f」选项，在手册中是这样解释的：

Flood ping. For every ECHO_REQUEST sent a period “.” is printed, while for ever ECHO_REPLY received a backspace is printed.  This provides a rapid display of how many packets are being dropped. If interval is not given, it sets interval to zero and outputs packets as fast as they come back or one hundred times per second, whichever is more. Only the super-user may use this option with zero interval.

简单点说：发送洪水请求，每个请求打印一个点，每个响应删除一个点。如果网络存在丢包，那么会呈现出一长串不断增加的点，简单易用，童叟无欺。

…
```

# TCP好文： TCP 的那些事儿 - 陈皓

* [TCP 的那些事儿（上）](http://coolshell.cn/articles/11564.html) or [TCP 的那些事儿（上） - local](resource/TCP 的那些事儿（上） _ 酷 壳 - CoolShell.cn.html)
* [TCP 的那些事儿（下）](http://coolshell.cn/articles/11609.html) or [TCP 的那些事儿（下） - local](resource/TCP 的那些事儿（下） _ 酷 壳 - CoolShell.cn.html)



