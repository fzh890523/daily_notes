
# 踩坑：spring部分bean销毁导致bean内线程退出问题

## 经过
1. spring load bean失败
`org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'slaRawLogKafkaInboundChannelAdapter.source': Cannot resolve reference to bean 'slaRawLogConsumerContext' while setting constructor argument; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'slaRawLogConsumerContext': Cannot create inner bean '(inner bean)#7f5fcfe9' of type [org.springframework.integration.kafka.support.ConsumerConfiguration] while setting bean property 'consumerConfigurations' with key [mygroup]; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name '(inner bean)#7f5fcfe9': Cannot create inner bean '(inner bean)#552fee7a' of type [org.springframework.integration.kafka.support.ConsumerMetadata] while setting constructor argument; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name '(inner bean)#552fee7a': Cannot resolve reference to bean 'slaSvcCounterDecoder' while setting bean property 'valueDecoder'; nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No bean named 'slaSvcCounterDecoder' is defined`
2. spring shutdown销毁已有bean
3. 已有某processor bean的`@PreDestroy`的fin方法里关闭了线程
4. 其他processor bean的线程依赖`AtomicBean closeFlag`而该flag只有在`SIGINT`/`SIGTERM`信号时才会设置为true，于是没有退出

主要问题也还是在于spring失败的日志没有打出来... = =

# 踩坑： 因为项目使用log4j2导致没有spring使用的log4j的配置从而无法打日志
`No appenders could be found for logger (org.springframework.web.context.support.StandardServletEnvironment).`

如上。 这样的话spring的报错就没法打出来，影响问题定位。

## 处理
加一个log4j的配置文件`log4j.properties`。
配置类似：

```
log4j.rootLogger=DEBUG, console
log4j.appender.console=org.apache.log4j.ConsoleAppender
log4j.appender.console.layout=org.apache.log4j.PatternLayout
log4j.appender.console.layout.conversionPattern=%5p [%t] (%F:%L) - %m%n
```


# 踩坑： 自定义的bean没有配置bean name然后跟spring已有bean冲突

```
framework.beans.factory.BeanDefinitionStoreException: Unexpected exception parsing XML document from class path resource [applicationContext.xml]; nested exception is org.springframework.context.annotation.ConflictingBeanDefinitionException: Annotation-specified bean name 'monitor' for bean class [com.bilibili.trafficstats.api.http.Monitor] conflicts with existing, non-compatible bean definition of same name and class [com.bilibili.slastats.api.http.Monitor]
    at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.doLoadBeanDefinitions(XmlBeanDefinitionReader.java:414)
    at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.loadBeanDefinitions(XmlBeanDefinitionReader.java:336)
    at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.loadBeanDefinitions(XmlBeanDefinitionReader.java:304)
    at org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:181)
    at org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:217)
    at org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:188)
    at org.springframework.context.annotation.ConfigurationClassBeanDefinitionReader.loadBeanDefinitionsFromImportedResources(ConfigurationClassBeanDefinitionReader.java:346)
    at org.springframework.context.annotation.ConfigurationClassBeanDefinitionReader.loadBeanDefinitionsForConfigurationClass(ConfigurationClassBeanDefinitionReader.java:142)
    at org.springframework.context.annotation.ConfigurationClassBeanDefinitionReader.loadBeanDefinitions(ConfigurationClassBeanDefinitionReader.java:116)
    at org.springframework.context.annotation.ConfigurationClassPostProcessor.processConfigBeanDefinitions(ConfigurationClassPostProcessor.java:333)
    at org.springframework.context.annotation.ConfigurationClassPostProcessor.postProcessBeanDefinitionRegistry(ConfigurationClassPostProcessor.java:243)
    at org.springframework.context.support.PostProcessorRegistrationDelegate.invokeBeanDefinitionRegistryPostProcessors(PostProcessorRegistrationDelegate.java:273)
    at org.springframework.context.support.PostProcessorRegistrationDelegate.invokeBeanFactoryPostProcessors(PostProcessorRegistrationDelegate.java:98)
    at org.springframework.context.support.AbstractApplicationContext.invokeBeanFactoryPostProcessors(AbstractApplicationContext.java:681)
    at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:523)
    at org.springframework.boot.context.embedded.EmbeddedWebApplicationContext.refresh(EmbeddedWebApplicationContext.java:122)
    at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:759)
    at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:369)
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:313)
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1185)
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1174)
    at com.bilibili.slastats.starter.SpringBootStarter.main(SpringBootStarter.java:27)
Caused by: org.springframework.context.annotation.ConflictingBeanDefinitionException: Annotation-specified bean name 'monitor' for bean class [com.bilibili.trafficstats.api.http.Monitor] conflicts with existing, non-compatible bean definition of same name and class [com.bilibili.slastats.api.http.Monitor]
    at org.springframework.context.annotation.ClassPathBeanDefinitionScanner.checkCandidate(ClassPathBeanDefinitionScanner.java:320)
    at org.springframework.context.annotation.ClassPathBeanDefinitionScanner.doScan(ClassPathBeanDefinitionScanner.java:259)
    at org.springframework.context.annotation.ComponentScanBeanDefinitionParser.parse(ComponentScanBeanDefinitionParser.java:87)
    at org.springframework.beans.factory.xml.NamespaceHandlerSupport.parse(NamespaceHandlerSupport.java:74)
    at org.springframework.beans.factory.xml.BeanDefinitionParserDelegate.parseCustomElement(BeanDefinitionParserDelegate.java:1411)
    at org.springframework.beans.factory.xml.BeanDefinitionParserDelegate.parseCustomElement(BeanDefinitionParserDelegate.java:1401)
    at org.springframework.beans.factory.xml.DefaultBeanDefinitionDocumentReader.parseBeanDefinitions(DefaultBeanDefinitionDocumentReader.java:172)
    at org.springframework.beans.factory.xml.DefaultBeanDefinitionDocumentReader.doRegisterBeanDefinitions(DefaultBeanDefinitionDocumentReader.java:142)
    at org.springframework.beans.factory.xml.DefaultBeanDefinitionDocumentReader.registerBeanDefinitions(DefaultBeanDefinitionDocumentReader.java:94)
    at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.registerBeanDefinitions(XmlBeanDefinitionReader.java:508)
    at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.doLoadBeanDefinitions(XmlBeanDefinitionReader.java:392)
    ... 21 more
Disconnected from the target VM, address: '127.0.0.1:52586', transport: 'socket'
```

如上。 `Monitor` 这个名字太短了，根据bean命名格式被spring取名为`monitor`于是...

改一下名字就好。
同时注意如果有多个的话不要重名。可以叫 `AAMonitorApi`, `BBMonitorApi` 等等。


# 踩坑： spring component-scan使用通配符时不包含本包
```xml
    <context:component-scan base-package="com.bilibili.slastats.api.*"
                            use-default-filters="false">
    </context:component-scan>
```
如上。 api.XXApi的controller bean是不会被扫描到的，因为不会扫api包，而是扫api下的子包。

# 踩坑： 根分区满了却没找到占用磁盘的文件
```
fwdjc-db:/:# df -h
Filesystem      Size  Used Avail Use% Mounted on
/dev/sda1        28G   28G   20K 100% /
udev             10M     0   10M   0% /dev
tmpfs           1.6G   57M  1.6G   4% /run
tmpfs           4.0G  8.0K  4.0G   1% /dev/shm
tmpfs           5.0M     0  5.0M   0% /run/lock
tmpfs           4.0G     0  4.0G   0% /sys/fs/cgroup
/dev/sda3        81G   39G   42G  48% /ssd
/dev/sdb1       300G   22G  278G   8% /data
```

`du -sh /*` 发现根分区下的文件加起来才三个G左右。
于是... 怀疑是已删除的文件句柄没有被释放。
看了下，果然...

```
fwdjc-db:/:# lsof -nP | grep '(deleted)'
mysqld     2810            mysql    5u      REG               0,17           0      47538 /dev/shm/ibI4NWyZ (deleted)
mysqld     2810            mysql    6u      REG               0,17         255      47539 /dev/shm/ib6UC9BV (deleted)
mysqld     2810            mysql    7u      REG               0,17           0      47540 /dev/shm/ib5symFR (deleted)
mysqld     2810            mysql    8u      REG               0,17           0      47541 /dev/shm/ibtgwMSN (deleted)
mysqld     2810            mysql   44u      REG               0,17           0      46884 /dev/shm/ibhE65ai (deleted)
mysqld     2810            mysql 3995w      REG                8,1 26442244096  103016091 /tmp/sql.log (deleted)
mysqld     2810  2819      mysql    5u      REG               0,17           0      47538 /dev/shm/ibI4NWyZ (deleted)
mysqld     2810  2819      mysql    6u      REG               0,17         255      47539 /dev/shm/ib6UC9BV (deleted)
mysqld     2810  2819      mysql    7u      REG               0,17           0      47540 /dev/shm/ib5symFR (deleted)
mysqld     2810  2819      mysql    8u      REG               0,17           0      47541 /dev/shm/ibtgwMSN (deleted)
mysqld     2810  2819      mysql   44u      REG               0,17           0      46884 /dev/shm/ibhE65ai (deleted)
mysqld     2810  2819      mysql 3995w      REG                8,1 26442244096  103016091 /tmp/sql.log (deleted)
...
```

重启mysql后ok。


# 踩坑： 测试发送消息到kafka后提示发送成功而实际kafka无数据
`Partition  Latest Offset` 发送（了100条）后offset还是0（不变）。
猜测是buf了。


```java
    @Override
    protected boolean doSend(Message<?> message, long timeout) {
        Assert.notNull(message, "'message' must not be null");
        try {
            if (this.queue instanceof BlockingQueue) {
                BlockingQueue<Message<?>> blockingQueue = (BlockingQueue<Message<?>>) this.queue;
                if (timeout > 0) {
                    return blockingQueue.offer(message, timeout, TimeUnit.MILLISECONDS);
                }
                if (timeout == 0) {
                    return blockingQueue.offer(message);
                }
                blockingQueue.put(message);
                return true;
            }
```

怕是没触发flush，于是一次发了300条，还是这样。
在代码后加了个sleep 5秒。 ok。
```java
    @Test
    public void TestSend() {
        getRawLogs(300).forEach(this::sendRawLog);
        try {
            Thread.sleep(5000);
        } catch (InterruptedException e) {
            e.printStackTrace();
        }
    }
```

## 疑问
这个怎么主动flush？ 停服务的时候丢数据这事让人不愉快啊。

## 其他问题
同时也发现，high-level api在consume msg/commit offset时也是批量进行的，也即应用层还没poll msg时其实已经在cacahe里同时offset也提交了。 如果此时发现未捕获异常或crash，那就算丢了。

# 踩坑： java String.split(String regExp) 竟然参数里是正则还支持 "||"
`api.bilibili.com||/x/v2/archive||500||1477469941683||50||192.16.1.4||`
```java
String[] splits = log.split("\\|\\|");
```
坑死了，原先是`String[] splits = log.split("||");`，... 被split成一个个的字符了。


# 踩坑： HBase java client竟然不检查表是否存在，同时<del>也不报错</del>
```java
        Increment increment = m.toIncrement();
        Table t = connection.getTable(TableName.valueOf(m.table()));
        t.increment(increment);
```
对于不存在的表，这里返回 non-null 的t。 同时 `t.increment` 操作不报错。 日了汪了。

啊，发现是自己弄错了。 日志报错打到别的log文件去了。 sorry。 mark一下。
至于不检查表存在，之前好像看过文档提到这个change。 算是计划内的。


# 踩坑： 因为hbase新RS节点host没配导致访问超时而client不断重试从而阻塞在一次操作中
发现有一个数据流没有更新了，又没有报错。
打断点代码还是跑不到（save逻辑），而又看到这个group已经消费了新数据。

判断应该是阻塞在某处。

## 小技巧
idea pause后看stack frame的变量时会带上对象id，这个可以用来区分两个不同的对象，也即可以用来区分两次执行到代码某位置。
这样的话可以用间隔几秒的两个pause来判断是否阻塞。
> 有一些阻塞是阻塞在“流程”中而不是某个函数/系统调用，所以直接看不能直观看出阻塞了

通过该方式发现在做一次increment时一直wait、retry、locate region，所以猜测...
后来定位到是因为该region所在的RS的host没配。

# 遇到问题kafka consumer连zk：can create message streams at most once

```
DEBUG [task-scheduler-7] (AbstractMessageChannel.java:430) - postSend (sent=true) on channel 'errorChannel', message: ErrorMessage [payload=kafka.common.MessageStreamsExistException: ZookeeperConsumerConnector can create message streams at most once, headers={id=59cff907-bb35-79ea-44e3-12e1d271d18e, timestamp=1477480512402}]
DEBUG [task-scheduler-8] (AbstractMessageHandler.java:115) - _org.springframework.integration.errorLogger.handler received message: ErrorMessage [payload=kafka.common.MessageStreamsExistException: ZookeeperConsumerConnector can create message streams at most once, headers={id=a76ac86a-b06b-63fd-8a9a-d1576dacb9de, timestamp=1477480512402}]
ERROR [task-scheduler-2] (LoggingHandler.java:184) - kafka.common.MessageStreamsExistException: ZookeeperConsumerConnector can create message streams at most once
    at kafka.javaapi.consumer.ZookeeperConsumerConnector.createMessageStreams(ZookeeperConsumerConnector.scala:79)
    at org.springframework.integration.kafka.support.ConsumerConfiguration.createMessageStreamsForTopic(ConsumerConfiguration.java:238)
    at org.springframework.integration.kafka.support.ConsumerConfiguration.createConsumerMessageStreams(ConsumerConfiguration.java:227)
    at org.springframework.integration.kafka.support.ConsumerConfiguration.receive(ConsumerConfiguration.java:99)
    at org.springframework.integration.kafka.support.KafkaConsumerContext.receive(KafkaConsumerContext.java:76)
    at org.springframework.integration.kafka.inbound.KafkaHighLevelConsumerMessageSource.receive(KafkaHighLevelConsumerMessageSource.java:42)
    at org.springframework.integration.endpoint.SourcePollingChannelAdapter.receiveMessage(SourcePollingChannelAdapter.java:209)
    at org.springframework.integration.endpoint.AbstractPollingEndpoint.doPoll(AbstractPollingEndpoint.java:245)
    at org.springframework.integration.endpoint.AbstractPollingEndpoint.access$000(AbstractPollingEndpoint.java:58)
    at org.springframework.integration.endpoint.AbstractPollingEndpoint$1.call(AbstractPollingEndpoint.java:190)
    at org.springframework.integration.endpoint.AbstractPollingEndpoint$1.call(AbstractPollingEndpoint.java:186)
    at org.springframework.integration.endpoint.AbstractPollingEndpoint$Poller$1.run(AbstractPollingEndpoint.java:353)
    at org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:55)
    at org.springframework.core.task.SyncTaskExecutor.execute(SyncTaskExecutor.java:50)
    at org.springframework.integration.util.ErrorHandlingTaskExecutor.execute(ErrorHandlingTaskExecutor.java:51)
    at org.springframework.integration.endpoint.AbstractPollingEndpoint$Poller.run(AbstractPollingEndpoint.java:344)
    at org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:54)
    at org.springframework.scheduling.concurrent.ReschedulingRunnable.run(ReschedulingRunnable.java:81)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)

ERROR [task-scheduler-8] (LoggingHandler.java:184) - kafka.common.MessageStreamsExistException: ZookeeperConsumerConnector can create message streams at most once
    at kafka.javaapi.consumer.ZookeeperConsumerConnector.createMessageStreams(ZookeeperConsumerConnector.scala:79)
    at org.springframework.integration.kafka.support.ConsumerConfiguration.createMessageStreamsForTopic(ConsumerConfiguration.java:238)
    at org.springframework.integration.kafka.support.ConsumerConfiguration.createConsumerMessageStreams(ConsumerConfiguration.java:227)
    at org.springframework.integration.kafka.support.ConsumerConfiguration.receive(ConsumerConfiguration.java:99)
    at org.springframework.integration.kafka.support.KafkaConsumerContext.receive(KafkaConsumerContext.java:76)
    at org.springframework.integration.kafka.inbound.KafkaHighLevelConsumerMessageSource.receive(KafkaHighLevelConsumerMessageSource.java:42)
    at org.springframework.integration.endpoint.SourcePollingChannelAdapter.receiveMessage(SourcePollingChannelAdapter.java:209)
    at org.springframework.integration.endpoint.AbstractPollingEndpoint.doPoll(AbstractPollingEndpoint.java:245)
    at org.springframework.integration.endpoint.AbstractPollingEndpoint.access$000(AbstractPollingEndpoint.java:58)
    at org.springframework.integration.endpoint.AbstractPollingEndpoint$1.call(AbstractPollingEndpoint.java:190)
    at org.springframework.integration.endpoint.AbstractPollingEndpoint$1.call(AbstractPollingEndpoint.java:186)
    at org.springframework.integration.endpoint.AbstractPollingEndpoint$Poller$1.run(AbstractPollingEndpoint.java:353)
    at org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:55)
    at org.springframework.core.task.SyncTaskExecutor.execute(SyncTaskExecutor.java:50)
    at org.springframework.integration.util.ErrorHandlingTaskExecutor.execute(ErrorHandlingTaskExecutor.java:51)
    at org.springframework.integration.endpoint.AbstractPollingEndpoint$Poller.run(AbstractPollingEndpoint.java:344)
    at org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:54)
    at org.springframework.scheduling.concurrent.ReschedulingRunnable.run(ReschedulingRunnable.java:81)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)

DEBUG [task-scheduler-2] (AbstractMessageChannel.java:430) - postSend (sent=true) on channel 'errorChannel', message: ErrorMessage [payload=kafka.common.MessageStreamsExistException: ZookeeperConsumerConnector can create message streams at most once, headers={id=6b032d09-e515-8c8f-ccb4-15b8bf10db0b, timestamp=1477480512402}]
DEBUG [task-scheduler-8] (AbstractMessageChannel.java:430) - postSend (sent=true) on channel 'errorChannel', message: ErrorMessage [payload=kafka.common.MessageStreamsExistException: ZookeeperConsumerConnector can create message streams at most once, headers={id=a76ac86a-b06b-63fd-8a9a-d1576dacb9de, timestamp=1477480512402}]
Disconnected from the target VM, address: '127.0.0.1:65249', transport: 'socket'
```

概率出现，概率还蛮大，百分之几十。 重启碰运气解决。 = =

## 分析
看起来像是zk上的节点延迟...

## 结果
未决

# 踩坑： spring bean xml定义时配错了ref的bean导致数据发到其他的channel里
低级错误，但还挺难查，要多留意。
```xml
    <int-kafka:inbound-channel-adapter
            id="ipCounterKafkaInboundChannelAdapter" kafka-consumer-context-ref="trafficIPCounterConsumerContext"
            auto-startup="true" channel="trafficIPCounterFromKafkaChannel">
        <int:poller fixed-delay="10" time-unit="MILLISECONDS"
                    max-messages-per-poll="5"/>
    </int-kafka:inbound-channel-adapter>
```

# 踩坑： java的byte是int8而不是uint8，so...
比如把ip解析为byte[4]然后做format的时候就可能被坑。
需要：
```java
return String.format("%d.%d.%d.%d", addr[0] & 0xFF, addr[1] & 0xFF, addr[2] & 0xFF, addr[3] & 0xFF);
```
这个 `0xFF` ，估计先转了int然后取低位，符号位被干掉了。

